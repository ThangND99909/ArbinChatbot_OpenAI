import logging
from langchain.chains import LLMChain
from langchain_core.runnables import Runnable
from langchain_core.callbacks import CallbackManagerForChainRun
from langchain.prompts import PromptTemplate
from ai_core.prompts import QA_PROMPT_TEMPLATE
from typing import Dict, Union, Any, Optional, List
from openai import OpenAI  # OpenAI v1.0.0+
import os


# ====== T·∫¢I BI·∫æN M√îI TR∆Ø·ªúNG (.env) ======

logger = logging.getLogger(__name__)

OPENAI_API_KEY = None  # S·∫Ω ƒë∆∞·ª£c set sau



# =========================================================
# =============== L·ªöP WRAPPER CHO OPENAI LLM ===============
# =========================================================
class OpenAILLM(Runnable):
    """
    Wrapper cho OpenAI (v1.0.0+) ƒë·ªÉ s·ª≠ d·ª•ng trong LangChain
    K·∫ø th·ª´a t·ª´ Runnable ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi LLMChain
    """

    def __init__(self, model: str = "gpt-4o-mini", temperature: float = 0.2):
        self.model = model
        self.temperature = temperature
        self.client = None
        self._initialize_client()
        
    def _initialize_client(self):
        """Kh·ªüi t·∫°o OpenAI client v·ªõi API key t·ª´ environment"""
        try:
            from dotenv import load_dotenv
            load_dotenv()
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key or api_key == "your_openai_api_key_here":
                logger.warning("OpenAI API key not properly configured")
                print("‚ö†Ô∏è WARNING: Please set OPENAI_API_KEY in .env file")
                api_key = "placeholder"  # For initialization without crashing
            
            self.client = OpenAI(api_key=api_key)
            logger.info(f"‚úÖ OpenAI client initialized with model: {self.model}")
            print(f"‚úÖ OpenAILLM initialized with model: {self.model}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize OpenAI client: {e}")
            print(f"‚ùå OpenAILLM initialization error: {e}")
            self.client = None

    def invoke(self, inputs: Union[str, Dict], config=None, **kwargs) -> str:
        """
        Ph∆∞∆°ng th·ª©c ch√≠nh ƒë·ªÉ g·ªçi OpenAI model.
        """
        
        if "stop" in kwargs:
            kwargs.pop("stop")

        # X·ª≠ l√Ω ƒë·∫ßu v√†o
        if isinstance(inputs, dict):
            if 'question' in inputs and 'language' in inputs:
                prompt = f"Question: {inputs['question']}\nLanguage: {inputs['language']}"
            elif 'context' in inputs and 'question' in inputs:
                prompt = QA_PROMPT_TEMPLATE.format(
                    context=inputs['context'],
                    question=inputs['question']
                )
            else:
                prompt = "\n".join(f"{k}: {v}" for k, v in inputs.items())
        else:
            prompt = str(inputs)

        # Ki·ªÉm tra client
        if not self.client:
            self._initialize_client()
            if not self.client:
                error_msg = "OpenAI client not available. Please check API key configuration."
                logger.error(error_msg)
                # RAISE EXCEPTION, kh√¥ng tr·∫£ v·ªÅ string
                raise Exception(error_msg)

        # ================= G·ª¨I REQUEST ƒê·∫æN OPENAI API =================
        try:
            # ====== T·ª∞ ƒê·ªòNG GI·ªöI H·∫†N ƒê·ªò D√ÄI C√ÇU TR·∫¢ L·ªúI ======
            # ∆Øu ti√™n ng·∫Øn g·ªçn h∆°n t√πy v√†o lo·∫°i t√°c v·ª•
            max_output_tokens = 500  # m·∫∑c ƒë·ªãnh
            if isinstance(inputs, dict):
                if "intent" in inputs.get("task", "").lower():
                    max_output_tokens = 150
                elif "entity" in inputs.get("task", "").lower():
                    max_output_tokens = 200
                elif "comparison" in inputs.get("task", "").lower():
                    max_output_tokens = 700
                elif "qa" in inputs.get("task", "").lower():
                    max_output_tokens = 500
                elif "support" in inputs.get("task", "").lower():
                    max_output_tokens = 400
                else:
                    # N·∫øu c√≥ context d√†i, gi·∫£m b·ªõt ƒë·ªÉ ti·∫øt ki·ªám token
                    context_len = len(inputs.get("context", "")) if "context" in inputs else 0
                    max_output_tokens = 300 if context_len < 2000 else 200

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                max_tokens=max_output_tokens
            )
            
            # ====== LOG TH√îNG TIN S·ª¨ D·ª§NG TOKEN =====
            if hasattr(response, "usage"):
                usage = response.usage
                print(f"üî¢ Tokens used: prompt={usage.prompt_tokens}, completion={usage.completion_tokens}, total={usage.total_tokens}")
            # ================= X·ª¨ L√ù K·∫æT QU·∫¢ TR·∫¢ V·ªÄ =================
            if response.choices and len(response.choices) > 0:
                result = response.choices[0].message.content.strip()
                logger.debug(f"OpenAI response received, length: {len(result)}")
            else:
                error_msg = "OpenAI returned empty response"
                logger.warning(error_msg)
                raise Exception(error_msg)
                
        except Exception as e:
            error_msg = str(e)
            logger.error(f"OpenAI API error: {error_msg}")
            print(f"‚ùå OpenAI API error: {error_msg}")
            
            # QUAN TR·ªåNG: RAISE EXCEPTION, kh√¥ng tr·∫£ v·ªÅ string
            # NLUProcessor c·∫ßn b·∫Øt exception n√†y
            raise Exception(f"OpenAI API error: {error_msg}")

        logger.debug(f"OpenAI response extracted: {result[:200]}...")
        return result

    # Ph∆∞∆°ng th·ª©c _call ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi giao di·ªán Runnable c·ªßa LangChain
    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:
        return self.invoke(prompt)

    # X√°c ƒë·ªãnh ki·ªÉu input/output cho Runnable
    @property
    def InputType(self):
        return str

    @property 
    def OutputType(self):
        return str
    
    def __repr__(self):
        return f"OpenAILLM(model='{self.model}', temperature={self.temperature})"


# =========================================================
# ===== L·ªöP SIMPLELLMMANAGER ‚Äî FALLBACK KHI OPENAI L·ªñI ====
# =========================================================
class SimpleLLMManager(Runnable):
    """
    Fallback LLM ƒë∆°n gi·∫£n n·∫øu OpenAI kh√¥ng ho·∫°t ƒë·ªông
    D√πng ƒë·ªÉ ƒë·∫£m b·∫£o h·ªá th·ªëng kh√¥ng b·ªã crash
    Ho√†n to√†n t∆∞∆°ng th√≠ch v·ªõi LangChain Runnable interface
    """

    def __init__(self, model: str = "simple-fallback", temperature: float = 0.1):
        logger.warning(f"Using simple fallback LLM: {model}")
        print(f"‚ö†Ô∏è Using simple fallback LLM: {model}")
        self.model = model
        self.temperature = temperature
        self.is_fallback = True  # Flag ƒë·ªÉ nh·∫≠n bi·∫øt ƒëang d√πng fallback

    # ========== CORE RUNNABLE INTERFACE METHODS ==========
    
    def invoke(self, inputs: Union[str, Dict], config: Optional[Dict] = None, **kwargs) -> str:
        """
        Implement invoke method v·ªõi ƒë√∫ng signature c·ªßa Runnable
        LangChain s·∫Ω g·ªçi method n√†y v·ªõi config parameter
        """
        # Debug logging
        print(f"üîß SimpleLLMManager.invoke() called")
        print(f"   Input type: {type(inputs)}")
        if isinstance(inputs, dict):
            print(f"   Input keys: {list(inputs.keys())}")
        else:
            print(f"   Input: {str(inputs)[:100]}...")
        
        # X·ª≠ l√Ω inputs theo c√°c tr∆∞·ªùng h·ª£p
        if isinstance(inputs, dict):
            # TR∆Ø·ªúNG H·ª¢P 1: NLU Intent Detection
            if 'question' in inputs and 'language' in inputs:
                question = inputs['question']
                language = inputs['language']
                print(f"   NLU Intent Detection format detected")
                
                # Tr·∫£ v·ªÅ JSON h·ª£p l·ªá cho intent detection
                return '{"intent": "unknown", "confidence": 0.5, "alternative_intents": [], "explanation": "Using fallback LLM for intent detection"}'
            
            # TR∆Ø·ªúNG H·ª¢P 2: NLU Entity Extraction
            elif 'question' in inputs:
                question = inputs['question']
                print(f"   NLU Entity Extraction format detected")
                
                # Tr·∫£ v·ªÅ JSON h·ª£p l·ªá cho entity extraction
                return '''{
  "entities": {
    "product_names": [],
    "technical_info": [],
    "applications": [],
    "features": [],
    "issues": [],
    "software": [],
    "locations": []
  },
  "confidence": 0.4,
  "extraction_notes": "Fallback entity extraction - no entities detected"
}'''
            
            # TR∆Ø·ªúNG H·ª¢P 3: QA Chain (context + question)
            elif 'context' in inputs and 'question' in inputs:
                context = inputs['context'][:200] if inputs['context'] else "No context"
                question = inputs['question']
                print(f"   QA Chain format detected")
                
                return f"""Based on context: {context}...

Question: {question}

Response: I'm currently using a fallback LLM. Please configure your OpenAI API key in the .env file for accurate responses about Arbin Instruments products.

Suggested next steps:
1. Check your .env file has OPENAI_API_KEY
2. Visit www.arbin.com for product information
3. Contact support@arbin.com for technical assistance"""
            
            # TR∆Ø·ªúNG H·ª¢P 4: Generic dict input
            else:
                return f"Fallback response for dictionary input: {str(inputs)[:100]}..."
        
        else:
            # TR∆Ø·ªúNG H·ª¢P 5: String input
            input_str = str(inputs)
            return f"""Fallback LLM Response:

You asked: "{input_str[:100]}..."

Note: I'm currently running in fallback mode. To get accurate information about Arbin Instruments products (BT series, MITS Pro, battery testing systems), please:

1. Configure OpenAI API key in .env file
2. Ensure vector store has relevant documents
3. Contact technical support if issues persist

For immediate assistance, email: support@arbin.com"""

    # Ph∆∞∆°ng th·ª©c _call ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi giao di·ªán Runnable c≈© c·ªßa LangChain
    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:
        print(f"üîß SimpleLLMManager._call() called")
        return self.invoke(prompt)

    # Batch invoke ƒë·ªÉ h·ªó tr·ª£ batch processing
    def batch(self, inputs: List[Union[str, Dict]], config: Optional[Dict] = None, **kwargs) -> List[str]:
        print(f"üîß SimpleLLMManager.batch() called with {len(inputs)} inputs")
        return [self.invoke(inp, config, **kwargs) for inp in inputs]

    # Stream method (optional, for streaming support)
    def stream(self, input: Union[str, Dict], config: Optional[Dict] = None, **kwargs):
        print(f"üîß SimpleLLMManager.stream() called")
        yield self.invoke(input, config, **kwargs)

    # X√°c ƒë·ªãnh ki·ªÉu input/output cho Runnable
    @property
    def InputType(self):
        from typing import Union
        return Union[str, Dict]

    @property 
    def OutputType(self):
        return str
    
    def __repr__(self):
        return f"SimpleLLMManager(model='{self.model}', temperature={self.temperature})"
    
    def __str__(self):
        return f"Fallback LLM Manager (model: {self.model})"
    
    # ========== COMPATIBILITY METHODS ==========
    
    @property
    def llm(self):
        """Tr·∫£ v·ªÅ self ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi interface c·ªßa LangChain"""
        return self

    def generate_response(self, question: str, context: str = "") -> str:
        """
        Sinh ph·∫£n h·ªìi ƒë∆°n gi·∫£n (m√¥ ph·ªèng LLM th·∫≠t)
        Gi·ªØ nguy√™n cho backward compatibility
        """
        if context:
            return f"""D·ª±a tr√™n th√¥ng tin c√≥ s·∫µn: {context[:200]}...

C√¢u h·ªèi: {question}

(L∆∞u √Ω: ƒêang s·ª≠ d·ª•ng fallback LLM, vui l√≤ng c·∫•u h√¨nh OpenAI API key trong file .env ƒë·ªÉ c√≥ c√¢u tr·∫£ l·ªùi ch√≠nh x√°c h∆°n)"""
        else:
            return f"""T√¥i nh·∫≠n ƒë∆∞·ª£c c√¢u h·ªèi: '{question}'.

(L∆∞u √Ω: ƒêang s·ª≠ d·ª•ng fallback LLM, vui l√≤ng c·∫•u h√¨nh OpenAI API key trong file .env ƒë·ªÉ c√≥ c√¢u tr·∫£ l·ªùi ch√≠nh x√°c h∆°n.

Th√¥ng tin v·ªÅ Arbin Instruments:
- Website: www.arbin.com
- H·ªó tr·ª£ k·ªπ thu·∫≠t: support@arbin.com
- S·∫£n ph·∫©m: BT series, MITS Pro, battery testing systems)"""

    def create_chain(self, name: str, prompt_template: str, input_vars: list):
        """
        T·∫°o chain gi·∫£ (mock) ƒë·ªÉ m√¥ ph·ªèng LangChain LLMChain
        """
        class MockChain:
            def __init__(self, name):
                self.name = name
            
            def invoke(self, inputs, config=None, **kwargs):
                print(f"üîß MockChain '{self.name}'.invoke() called")
                
                if isinstance(inputs, dict):
                    question = inputs.get('question', '')
                    context = inputs.get('context', '')
                    return {
                        'text': f"Mock response for chain: {self.name}\n\nQuestion: {question}\n\nContext: {context[:100]}..."
                    }
                else:
                    return {
                        'text': f"Mock response for chain: {self.name}\n\nInput: {str(inputs)[:100]}..."
                    }
            
            def batch(self, inputs_list, config=None, **kwargs):
                return [self.invoke(inp, config, **kwargs) for inp in inputs_list]

        return MockChain(name)

    def run_chain(self, name: str, inputs: Dict) -> str:
        """Gi·∫£ l·∫≠p vi·ªác ch·∫°y chain"""
        print(f"üîß SimpleLLMManager.run_chain('{name}')")
        return f"Mock response from chain '{name}': {str(inputs)[:100]}..."

    def predict(self, prompt: str) -> str:
        """Ph·∫£n h·ªìi ƒë∆°n gi·∫£n khi ch·ªâ c√≥ prompt"""
        print(f"üîß SimpleLLMManager.predict()")
        return f"Fallback LLM response: {prompt[:100]}..."
    
    # ========== ADDITIONAL HELPER METHODS ==========
    
    def get_model_info(self) -> Dict[str, Any]:
        """Tr·∫£ v·ªÅ th√¥ng tin model"""
        return {
            "model_type": "fallback",
            "model_name": self.model,
            "temperature": self.temperature,
            "is_fallback": True,
            "capabilities": ["text_generation", "intent_detection", "entity_extraction"]
        }
    
    def health_check(self) -> Dict[str, Any]:
        """Ki·ªÉm tra tr·∫°ng th√°i health c·ªßa LLM"""
        return {
            "status": "operational",
            "mode": "fallback",
            "message": "Fallback LLM is running. Configure OpenAI API for full functionality.",
            "model": self.model
        }


# =========================================================
# =============== L·ªöP QU·∫¢N L√ù CH√çNH ‚Äî LLM MANAGER ==========
# =========================================================
class LLMManager:
    """
    L·ªõp trung t√¢m qu·∫£n l√Ω LLM (OpenAI ho·∫∑c fallback)
    T∆∞∆°ng th√≠ch v·ªõi c·∫•u tr√∫c LLMChain c·ªßa LangChain
    """

    def __init__(self, model: str = "gpt-4o-mini", temperature: float = 0.1, use_openai: bool = True):
        self.model = model
        self.temperature = temperature
        self.use_openai = use_openai
        self.chains = {}

        # ===== Ki·ªÉm tra API key v√† kh·ªüi t·∫°o LLM =====
        print(f"\nüîÑ Initializing LLMManager...")
        print(f"   Model: {model}")
        print(f"   Use OpenAI: {use_openai}")
        
        # Load .env ƒë·ªÉ check API key
        from dotenv import load_dotenv
        load_dotenv()
        api_key = os.getenv("OPENAI_API_KEY")
        
        if use_openai and api_key and len(api_key) > 20 and not api_key.startswith("your_"):
            try:
                print("   Attempting to initialize OpenAI...")
                self.llm = OpenAILLM(model=model, temperature=temperature)
                logger.info(f"‚úÖ Using OpenAI LLM: {model}")
                print(f"‚úÖ LLMManager initialized with OpenAI: {model}")
            except Exception as e:
                print(f"‚ùå OpenAI init failed: {e}")
                print("üîÑ Switching to fallback LLM")
                self.llm = SimpleLLMManager(model=model, temperature=temperature)
        else:
            print("‚ö†Ô∏è OpenAI API key not found or invalid. Using fallback LLM.")
            self.llm = SimpleLLMManager(model=model, temperature=temperature)

        # ===== Kh·ªüi t·∫°o c√°c chain m·∫∑c ƒë·ªãnh =====
        try:
            self._init_default_chains()
            logger.info("‚úÖ Default LLM chains initialized successfully.")
        except Exception as e:
            logger.error(f"Failed to initialize default chains: {e}")
            print(f"‚ö†Ô∏è Warning: Could not initialize default chains ({e}).")

        # ===== Kh·ªüi t·∫°o c√°c chain m·∫∑c ƒë·ªãnh =====
        try:
            self._init_default_chains()
            logger.info("‚úÖ Default LLM chains initialized successfully.")
        except Exception as e:
            logger.error(f"Failed to initialize default chains: {e}")
            print(f"‚ö†Ô∏è Warning: Could not initialize default chains ({e}).")

    def _init_default_chains(self):
        """
        Kh·ªüi t·∫°o chain m·∫∑c ƒë·ªãnh: QA (Question Answering)
        """
        qa_prompt = PromptTemplate(
            template=QA_PROMPT_TEMPLATE,
            input_variables=["context", "question"]
        )

        try:
            self.qa_chain = LLMChain(
                llm=self.llm,
                prompt=qa_prompt,
                verbose=False
            )
            logger.info("‚úÖ Initialized LLM chains")
        except Exception as e:
            logger.warning(f"Failed to create LLMChain: {e}")
            print(f"‚ö†Ô∏è LLMChain creation warning: {e}")
            # Fallback chain
            class SimpleQAClient:
                def invoke(self, inputs):
                    context = inputs.get("context", "")
                    question = inputs.get("question", "")
                    return {
                        'text': f"Simple QA response:\nContext: {context[:100]}...\nQuestion: {question}"
                    }
            self.qa_chain = SimpleQAClient()

    def generate_response(self, question: str, context: str = "") -> str:
        """
        H√†m t·∫°o ph·∫£n h·ªìi ch√≠nh (ƒë∆∞·ª£c g·ªçi b·ªüi chatbot)
        - N·∫øu c√≥ context ‚Üí d√πng QA chain
        - N·∫øu kh√¥ng ‚Üí g·ªçi tr·ª±c ti·∫øp LLM
        """
        try:
            if context:
                # G·ªçi chain QA
                result = self.qa_chain.invoke({
                    "context": context,
                    "question": question
                })

                # ƒê·∫£m b·∫£o tr·∫£ v·ªÅ string
                if isinstance(result, dict) and 'text' in result:
                    return str(result['text']).strip()
                elif isinstance(result, str):
                    return result.strip()
                else:
                    return str(result).strip()
            else:
                # N·∫øu kh√¥ng c√≥ context, g·ªçi invoke tr·ª±c ti·∫øp
                return str(self.llm.invoke(question)).strip()

        except Exception as e:
            logger.error(f"Error in generate_response: {e}")
            print(f"‚ùå Error in generate_response: {e}")
            return str(f"Error: {str(e)[:200]}").strip()

    def create_chain(self, name: str, prompt_template: str, input_vars: list):
        """
        T·∫°o th√™m chain t√πy ch·ªânh m·ªõi
        (v√≠ d·ª•: chain cho sentiment, summarization,...)
        """
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=input_vars
        )

        try:
            chain = LLMChain(
                llm=self.llm,
                prompt=prompt,
                verbose=False
            )
            self.chains[name] = chain
            return chain
        except Exception as e:
            logger.error(f"Failed to create chain '{name}': {e}")
            # Return mock chain
            class MockChain:
                def invoke(self, inputs):
                    return {'text': f"Mock chain '{name}' response"}
            return MockChain()

    def run_chain(self, name: str, inputs: Dict) -> str:
        """
        Ch·∫°y chain theo t√™n ƒë√£ t·∫°o
        """
        chain = self.chains.get(name)
        if not chain:
            raise ValueError(f"Chain '{name}' not found")

        result = chain.invoke(inputs)
        return result['text']


# =========================================================
# =============== FACTORY FUNCTION =========================
# =========================================================
def get_llm_manager(use_openai: bool = True):
    """
    H√†m ti·ªán √≠ch ƒë·ªÉ kh·ªüi t·∫°o LLMManager
    D√πng ƒë·ªÉ d·ªÖ d√†ng thay ƒë·ªïi model ho·∫∑c fallback
    """
    return LLMManager(use_openai=use_openai)


