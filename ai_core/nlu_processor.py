# ============================================================
# NLU PROCESSOR CHO ARBIN INSTRUMENTS
# ------------------------------------------------------------
# T√°c d·ª•ng:
#  - Ph√¢n t√≠ch c√¢u h·ªèi ng∆∞·ªùi d√πng ƒë·ªÉ x√°c ƒë·ªãnh "intent" (√Ω ƒë·ªãnh)
#  - Tr√≠ch xu·∫•t "entities" (th·ª±c th·ªÉ nh∆∞ t√™n s·∫£n ph·∫©m, th√¥ng s·ªë, l·ªói, v.v.)
#  - H·ªó tr·ª£ ng·ªØ c·∫£nh h·ªôi tho·∫°i (Context Memory) ƒë·ªÉ duy tr√¨ m·∫°ch tr√≤ chuy·ªán
#  - K·∫øt h·ª£p AI (Gemini) + keyword fallback ƒë·ªÉ ƒë·∫£m b·∫£o ·ªïn ƒë·ªãnh
# ============================================================

import json
import re
import logging
import unicodedata
from typing import Dict, Any, List, Optional
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import BaseOutputParser
from ai_core.llm_chain import GeminiLLM
from .prompts import intent_prompt, entity_prompt
from .parsers import NLUOutputParser
import traceback

# ========================
# Logging setup
# ========================
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# ============================================================
# B·ªò NH·ªö NG·ªÆ C·∫¢NH ƒê∆†N GI·∫¢N (ContextMemory)
# ============================================================
class ContextMemory:
    """
    L∆∞u l·∫°i intent v√† entities g·∫ßn nh·∫•t m√† ng∆∞·ªùi d√πng ƒë√£ n√≥i.
    D√πng ƒë·ªÉ gi√∫p chatbot hi·ªÉu m·∫°ch h·ªôi tho·∫°i v√† duy tr√¨ ng·ªØ c·∫£nh.
    """
    def __init__(self):
        self.last_intent = None
        self.last_entities = {}
        self.last_product_mentioned = None  # S·∫£n ph·∫©m v·ª´a ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn trong h·ªôi tho·∫°i

    def update(self, intent: str, entities: Dict[str, Any]):
        """C·∫≠p nh·∫≠t intent v√† entities m·ªõi nh·∫•t"""
        if intent and intent != "unknown":
            self.last_intent = intent
        
        if entities:
            self.last_entities = entities
            # N·∫øu c√≥ s·∫£n ph·∫©m ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn, l∆∞u l·∫°i
            if entities.get("product_names"):
                self.last_product_mentioned = entities["product_names"][0]

    def get_context(self) -> Dict[str, Any]:
        """Tr·∫£ v·ªÅ ng·ªØ c·∫£nh hi·ªán t·∫°i g·ªìm intent, entity, s·∫£n ph·∫©m"""
        return {
            "last_intent": self.last_intent,
            "last_entities": self.last_entities,
            "last_product_mentioned": self.last_product_mentioned
        }

# ============================================================
# H√ÄM H·ªñ TR·ª¢ (TI·ªÄN X·ª¨ L√ù)
# ============================================================
def _strip_accents(s: str) -> str:
    """B·ªè d·∫•u ti·∫øng Vi·ªát ƒë·ªÉ so kh·ªõp keyword d·ªÖ h∆°n"""
    if not isinstance(s, str):
        return ""
    return "".join(c for c in unicodedata.normalize("NFD", s) if unicodedata.category(c) != "Mn")

def _contains_any(haystack: str, needles) -> bool:
    """Ki·ªÉm tra xem chu·ªói c√≥ ch·ª©a b·∫•t k·ª≥ t·ª´ kh√≥a n√†o trong danh s√°ch kh√¥ng"""
    return any(n in haystack for n in needles)

# ============================================================
# DANH S√ÅCH T·ª™ KH√ìA CHUY√äN BI·ªÜT CHO ARBIN
# ============================================================
# M·ª•c ƒë√≠ch: gi√∫p nh·∫≠n d·∫°ng intent v√† entity m√† kh√¥ng c·∫ßn AI (fallback mode)

# C√°c c·ª•m th∆∞·ªùng xu·∫•t hi·ªán trong c√¢u h·ªèi
QUESTION_TRIGGERS = [
    "l√† g√¨", "la gi", "gi·ªõi thi·ªáu", "gioi thieu", "th√¥ng tin", "thong tin",
    "bao nhi√™u", "bao nhieu", "v√¨ sao", "vi sao", "?", "how", "what", "which",
    "t·∫°i sao", "tai sao", "th·∫ø n√†o", "the nao", "c√°ch", "cach", "how to"
]

# T·ª´ kh√≥a s·∫£n ph·∫©m Arbin (v√≠ d·ª• BT2000, MITS, EV Test...)
PRODUCT_KEYWORDS = [
    "bt", "bt-", "lbt", "mbt", "mits", "mits pro", "ev test", "battery tester",
    "cell tester", "battery cycler", "test system", "arbin", "testing system",
    "hardware", "software", "win", "daq", "windaq", "console", "client"
]

# Thu·∫≠t ng·ªØ k·ªπ thu·∫≠t (technical keywords)
TECHNICAL_KEYWORDS = [
    "voltage", "current", "capacity", "power", "impedance", "resistance",
    "soc", "soh", "cycle", "charging", "discharging", "calibration",
    "accuracy", "resolution", "range", "channel", "frequency", "temperature",
    "measurement", "testing", "analysis", "data acquisition", "monitoring"
]

# T·ª´ kh√≥a li√™n quan h·ªó tr·ª£ k·ªπ thu·∫≠t
TECH_SUPPORT_KEYWORDS = [
    "error", "problem", "issue", "bug", "fix", "repair", "troubleshoot",
    "help", "how to", "why", "not working", "l·ªói", "s·ª± c·ªë", "kh√¥ng ho·∫°t ƒë·ªông",
    "h∆∞·ªõng d·∫´n", "c√°ch s·ª≠ d·ª•ng", "gi·∫£i quy·∫øt", "support", "assistance",
    "crash", "fail", "broken", "malfunction", "calibration error"
]

# T·ª´ kh√≥a v·ªÅ th√¥ng s·ªë k·ªπ thu·∫≠t
SPECIFICATION_KEYWORDS = [
    "spec", "specification", "parameter", "technical data", "feature",
    "capacity", "voltage", "current", "power", "accuracy", "resolution",
    "th√¥ng s·ªë", "ƒë·∫∑c t√≠nh", "t√≠nh nƒÉng", "d·∫£i ƒëo", "ƒë·ªô ch√≠nh x√°c", "range"
]

# T·ª´ kh√≥a gi√° c·∫£
PRICING_KEYWORDS = [
    "price", "cost", "quote", "quotation", "budget", "expensive", "cheap",
    "how much", "gi√°", "chi ph√≠", "b√°o gi√°", "ƒë·ªãnh gi√°", "kinh ph√≠",
    "affordable", "pricing", "estimate", "quotation"
]

# T·ª´ kh√≥a so s√°nh s·∫£n ph·∫©m
COMPARISON_KEYWORDS = [
    "compare", "versus", "vs", "difference", "different", "better",
    "best", "worst", "advantages", "disadvantages", "pros", "cons",
    "so s√°nh", "kh√°c bi·ªát", "∆∞u ƒëi·ªÉm", "nh∆∞·ª£c ƒëi·ªÉm", "t·ªët h∆°n", "h∆°n k√©m"
]

# T·ª´ kh√≥a v·ªÅ ·ª©ng d·ª•ng
APPLICATION_KEYWORDS = [
    "application", "use", "usage", "purpose", "suitable for", "scenario",
    "·ª©ng d·ª•ng", "s·ª≠ d·ª•ng", "m·ª•c ƒë√≠ch", "ph√π h·ª£p", "t√¨nh hu·ªëng", "lƒ©nh v·ª±c",
    "industry", "field", "purpose", "for what", "d√πng cho"
]

# ƒê·ªãa ƒëi·ªÉm ·ª©ng d·ª•ng (lab, nh√† m√°y,...)
LOCATION_TYPES = [
    "laboratory", "lab", "factory", "manufacturing", "research center",
    "university", "college", "test facility", "quality control",
    "production line", "ph√≤ng th√≠ nghi·ªám", "nh√† m√°y", "x∆∞·ªüng s·∫£n xu·∫•t",
    "trung t√¢m nghi√™n c·ª©u", "ƒë·∫°i h·ªçc", "c∆° s·ªü th·ª≠ nghi·ªám"
]

# ============================================================
# üß© NLU PROCESSOR CH√çNH CHO ARBIN
# ============================================================
class NLUProcessor:
    """
    X·ª≠ l√Ω to√†n b·ªô pipeline NLU:
    - Intent detection (ph√°t hi·ªán √Ω ƒë·ªãnh)
    - Entity extraction (tr√≠ch xu·∫•t th·ª±c th·ªÉ)
    - Context tracking (theo d√µi ng·ªØ c·∫£nh h·ªôi tho·∫°i)
    """

    def __init__(self, llm=None, memory_manager=None):
        """
        llm: m√¥ h√¨nh LLM (Gemini)
        memory_manager: ƒë·ªëi t∆∞·ª£ng qu·∫£n l√Ω h·ªôi tho·∫°i (ƒë·ªÉ l·∫•y intent, question, v.v.)
        """
        self.llm = llm or GeminiLLM()
        self.memory_manager = memory_manager
        self.memory = ContextMemory()  # B·ªô nh·ªõ ng·ªØ c·∫£nh c·ª•c b·ªô
        
        # T·∫°o chain cho Intent Detection
        self.intent_chain = LLMChain(
            llm=self.llm,
            prompt=intent_prompt,
            output_parser=NLUOutputParser(),
            output_key="answer"
        )
        
        # T·∫°o chain cho Entity Extraction
        self.entity_chain = LLMChain(
            llm=self.llm,
            prompt=entity_prompt,
            output_parser=NLUOutputParser(),
            output_key="answer"
        )
        
        logger.info("‚úÖ Arbin NLUProcessor initialized")

    # =======================================================
    # H√ÄM L·∫§Y TIN NH·∫ÆN ASSISTANT G·∫¶N NH·∫§T
    # =======================================================
    def _get_last_assistant_message(self, session_id: str) -> str:
        """Truy xu·∫•t ph·∫£n h·ªìi g·∫ßn nh·∫•t t·ª´ chatbot (d√πng cho context linking)"""
        if not self.memory_manager:
            return ""
        try:
            msgs = self.memory_manager.get_messages(session_id)
            for m in reversed(msgs):
                if hasattr(m, "type") and m.type != "human":
                    return getattr(m, "content", "") or ""
        except Exception as e:
            logger.debug(f"Kh√¥ng l·∫•y ƒë∆∞·ª£c last assistant message: {e}")
        return ""

    # =======================================================
    # PH√ÅT HI·ªÜN INTENT (Detect Intent)
    # =======================================================
    def detect_intent(self, question: str, language: str = "en", session_id: str = "default") -> Dict[str, Any]:
        """
        D·ª± ƒëo√°n √Ω ƒë·ªãnh c·ªßa ng∆∞·ªùi d√πng cho Arbin Instruments.
        K·∫øt h·ª£p AI (Gemini) + heuristic (keyword fallback)
        """
        try:
            # G·ªçi LLM chain ƒë·ªÉ d·ª± ƒëo√°n intent
            try:
                raw = self.intent_chain.invoke({"question": question, "language": language})
            except Exception:
                # N·∫øu l·ªói LLM ‚Üí fallback sang keyword
                return self._get_intent_by_keywords_fallback(question, session_id)
            
            # Chu·∫©n h√≥a output
            if isinstance(raw, dict):
                if 'answer' in raw:
                    output_text = raw['answer']
                elif 'text' in raw:
                    output_text = raw['text']
                else:
                    output_text = str(raw)
            else:
                output_text = str(raw)
            
            # Parse k·∫øt qu·∫£ t·ª´ LLM (d·∫°ng JSON)
            try:
                if isinstance(output_text, dict):
                    output_text = json.dumps(output_text, ensure_ascii=False)
                parsed = self.intent_chain.output_parser.parse(output_text)
                if isinstance(parsed, dict):
                    intent = parsed.get("intent", "unknown")
                    confidence = float(parsed.get("confidence", 0.0))
                else:
                    intent = "unknown"
                    confidence = 0.0
            except Exception:
                intent = "unknown"
                confidence = 0.0
            
            # L·∫•y intent v√† question g·∫ßn nh·∫•t t·ª´ memory_manager
            last_intent, last_question = "", ""
            if self.memory_manager:
                last_intent = self.memory_manager.get_last_intent(session_id)
                last_question = self.memory_manager.get_last_question(session_id)

            # Chu·∫©n h√≥a text ƒë·ªÉ match keywords
            t_lc = question.strip().lower()
            t_ascii = _strip_accents(t_lc)
            words = t_ascii.split()
            is_short = len(words) <= 4  # C√¢u ng·∫Øn c√≥ th·ªÉ c·∫ßn enrichment

            # Ki·ªÉm tra keyword ƒë·ªÉ tƒÉng ƒë·ªô tin c·∫≠y
            has_tech_support_kw = _contains_any(t_lc, TECH_SUPPORT_KEYWORDS) or _contains_any(t_ascii, TECH_SUPPORT_KEYWORDS)
            has_spec_kw = _contains_any(t_lc, SPECIFICATION_KEYWORDS) or _contains_any(t_ascii, SPECIFICATION_KEYWORDS)
            has_pricing_kw = _contains_any(t_lc, PRICING_KEYWORDS) or _contains_any(t_ascii, PRICING_KEYWORDS)
            has_comparison_kw = _contains_any(t_lc, COMPARISON_KEYWORDS) or _contains_any(t_ascii, COMPARISON_KEYWORDS)
            has_application_kw = _contains_any(t_lc, APPLICATION_KEYWORDS) or _contains_any(t_ascii, APPLICATION_KEYWORDS)
            has_product_kw = _contains_any(t_lc, PRODUCT_KEYWORDS) or _contains_any(t_ascii, PRODUCT_KEYWORDS)
            has_question_word = _contains_any(t_lc, QUESTION_TRIGGERS) or _contains_any(t_ascii, QUESTION_TRIGGERS)

            # =======================================================
            # N√¢ng c·∫•p intent d·ª±a tr√™n heuristic / keyword
            # =======================================================
            enriched_text = None
            
            # N·∫øu intent unknown ho·∫∑c confidence th·∫•p ‚Üí fallback theo t·ª´ kh√≥a
            if intent == "unknown" or confidence < 0.3:
                if has_tech_support_kw:
                    intent = "technical_support"
                    confidence = max(confidence, 0.7)
                elif has_spec_kw:
                    intent = "specification_request"
                    confidence = max(confidence, 0.7)
                elif has_pricing_kw:
                    intent = "pricing_inquiry"
                    confidence = max(confidence, 0.7)
                elif has_comparison_kw:
                    intent = "comparison_request"
                    confidence = max(confidence, 0.7)
                elif has_application_kw:
                    intent = "application_info"
                    confidence = max(confidence, 0.7)
                elif has_product_kw and has_question_word:
                    intent = "product_inquiry"
                    confidence = max(confidence, 0.6)
                elif has_product_kw:
                    intent = "product_inquiry"
                    confidence = max(confidence, 0.5)

            # N·∫øu c√¢u ng·∫Øn, g·∫Øn th√™m context tr∆∞·ªõc ƒë√≥ ƒë·ªÉ hi·ªÉu h∆°n
            if last_intent == "product_inquiry" and is_short and has_product_kw:
                intent = "product_inquiry"
                enriched_text = f"{last_question} {question}" if last_question else question
            
            elif last_intent == "product_inquiry" and has_spec_kw:
                intent = "specification_request"
                enriched_text = f"specifications of {self.memory.last_product_mentioned or 'the product'} {question}"

            if not enriched_text and is_short and last_question:
                enriched_text = f"{last_question} {question}"
            if enriched_text is None:
                enriched_text = question

            # C·∫≠p nh·∫≠t b·ªô nh·ªõ t·∫°m (context memory)
            if intent != "unknown":
                self.memory.update(intent, {})

            return {
                "intent": intent,
                "confidence": confidence,
                "last_intent": last_intent,
                "last_question": last_question,
                "last_product_mentioned": self.memory.last_product_mentioned,
                "enriched_text": enriched_text,
                "keywords_detected": {
                    "has_product": has_product_kw,
                    "has_technical": _contains_any(t_lc, TECHNICAL_KEYWORDS),
                    "has_specification": has_spec_kw,
                    "has_support": has_tech_support_kw,
                    "has_pricing": has_pricing_kw,
                    "has_comparison": has_comparison_kw,
                    "has_application": has_application_kw
                }
            }

        except Exception:
            return {
                "intent": "unknown",
                "confidence": 0.0,
                "last_intent": "",
                "last_question": "",
                "enriched_text": None,
                "keywords_detected": {}
            }

    # =======================================================
    # FALLBACK PH√ÅT HI·ªÜN INTENT B·∫∞NG KEYWORDS (KH√îNG C·∫¶N AI)
    # =======================================================
    def _get_intent_by_keywords_fallback(self, question: str, session_id: str) -> Dict[str, Any]:
        """
        Khi Gemini API b·ªã l·ªói ho·∫∑c timeout, h√†m n√†y s·∫Ω
        t·ª± ƒë·ªông x√°c ƒë·ªãnh intent ch·ªâ d·ª±a tr√™n t·ª´ kh√≥a.
        """
        logger.warning("Using keyword-only fallback intent detection")
        
        # Chu·∫©n h√≥a text (ch·ªØ th∆∞·ªùng + b·ªè d·∫•u)
        t_lc = question.strip().lower()
        t_ascii = _strip_accents(t_lc)
        
        # Ki·ªÉm tra t·ª´ng nh√≥m keyword
        has_product_kw = _contains_any(t_lc, PRODUCT_KEYWORDS) or _contains_any(t_ascii, PRODUCT_KEYWORDS)
        has_tech_support_kw = _contains_any(t_lc, TECH_SUPPORT_KEYWORDS) or _contains_any(t_ascii, TECH_SUPPORT_KEYWORDS)
        has_spec_kw = _contains_any(t_lc, SPECIFICATION_KEYWORDS) or _contains_any(t_ascii, SPECIFICATION_KEYWORDS)
        has_pricing_kw = _contains_any(t_lc, PRICING_KEYWORDS) or _contains_any(t_ascii, PRICING_KEYWORDS)
        has_comparison_kw = _contains_any(t_lc, COMPARISON_KEYWORDS) or _contains_any(t_ascii, COMPARISON_KEYWORDS)
        has_application_kw = _contains_any(t_lc, APPLICATION_KEYWORDS) or _contains_any(t_ascii, APPLICATION_KEYWORDS)
        has_question_word = _contains_any(t_lc, QUESTION_TRIGGERS) or _contains_any(t_ascii, QUESTION_TRIGGERS)
        
        # M·∫∑c ƒë·ªãnh intent ch∆∞a x√°c ƒë·ªãnh
        intent = "unknown"
        confidence = 0.5  # confidence trung b√¨nh cho rule-based
        
        # Ph√¢n lo·∫°i intent d·ª±a tr√™n nh√≥m t·ª´ kh√≥a
        if has_tech_support_kw:
            intent = "technical_support"
            confidence = 0.7
        elif has_spec_kw:
            intent = "specification_request"
            confidence = 0.7
        elif has_pricing_kw:
            intent = "pricing_inquiry"
            confidence = 0.7
        elif has_comparison_kw:
            intent = "comparison_request"
            confidence = 0.7
        elif has_application_kw:
            intent = "application_info"
            confidence = 0.7
        elif has_product_kw and has_question_word:
            intent = "product_inquiry"
            confidence = 0.6
        elif has_product_kw:
            intent = "product_inquiry"
            confidence = 0.5
        
        # L·∫•y th√¥ng tin ng·ªØ c·∫£nh tr∆∞·ªõc ƒë√≥ t·ª´ memory_manager (n·∫øu c√≥)
        last_intent, last_question = "", ""
        if self.memory_manager:
            last_intent = self.memory_manager.get_last_intent(session_id)
            last_question = self.memory_manager.get_last_question(session_id)
        
        # N·∫øu ph√°t hi·ªán c√≥ t√™n s·∫£n ph·∫©m, tr√≠ch xu·∫•t ra
        product_names = self._extract_product_names_from_text(question) if has_product_kw else []
        
        # Tr·∫£ k·∫øt qu·∫£ fallback
        return {
            "intent": intent,
            "confidence": confidence,
            "last_intent": last_intent,
            "last_question": last_question,
            "last_product_mentioned": product_names[0] if product_names else None,
            "enriched_text": question,
            "keywords_detected": {
                "has_product": has_product_kw,
                "has_technical": _contains_any(t_lc, TECHNICAL_KEYWORDS),
                "has_specification": has_spec_kw,
                "has_support": has_tech_support_kw,
                "has_pricing": has_pricing_kw,
                "has_comparison": has_comparison_kw,
                "has_application": has_application_kw,
                "has_question_word": has_question_word
            },
            "intent_override_applied": True  # ƒë√°nh d·∫•u d√πng rule-based
        }

    # =======================================================
    # TR√çCH XU·∫§T T√äN S·∫¢N PH·∫®M (Product Extraction)
    # =======================================================
    def _extract_product_names_from_text(self, text: str) -> List[str]:
        """
        D√≤ t√¨m t√™n s·∫£n ph·∫©m Arbin trong c√¢u h·ªèi
        V√≠ d·ª•: "BT2000", "MITS Pro", "Battery Tester"
        """
        products = []
        text_lower = text.lower()
        
        # C√°c pattern regex ph·ªï bi·∫øn cho t√™n s·∫£n ph·∫©m Arbin
        product_patterns = [
            r'bt[-\s]?\d+',          # BT-2000, BT 2000
            r'lbt[-\s]?\d*',         # LBT series
            r'mbt[-\s]?\d*',         # MBT series
            r'mits\s*(?:pro|x)?',    # MITS Pro, MITS X
            r'ev\s*(?:test|testing)?',  # EV test
            r'battery\s+tester',     # battery tester
            r'cell\s+tester',        # cell tester
            r'battery\s+cycler',     # battery cycler
        ]
        
        # D√≤ t·ª´ng pattern
        for pattern in product_patterns:
            matches = re.findall(pattern, text_lower, re.IGNORECASE)
            products.extend(matches)
        
        return products

    # =======================================================
    # TR√çCH XU·∫§T TH·ª∞C TH·ªÇ (Entity Extraction)
    # =======================================================
    def extract_entities(self, question: str, language: str = "en") -> Dict[str, Any]:
        """
        G·ªçi LLM ƒë·ªÉ ph√¢n t√≠ch c√¢u h·ªèi v√† tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ
        (v√≠ d·ª•: t√™n s·∫£n ph·∫©m, th√¥ng s·ªë, l·ªói, ph·∫ßn m·ªÅm, v.v.)
        """
        try:
            # Th·ª≠ g·ªçi LLM chain
            try:
                result = self.entity_chain.invoke({"question": question, "language": language})
            except Exception:
                # N·∫øu LLM l·ªói ‚Üí fallback keyword extraction
                return {
                    "entities": self._keyword_entity_extraction(question),
                    "confidence": 0.4,
                    "raw_output": ""
                }
            
            # Chu·∫©n h√≥a output t·ª´ LLM
            if isinstance(result, dict):
                if 'answer' in result:
                    output_text = result['answer']
                elif 'text' in result:
                    output_text = result['text']
                else:
                    output_text = str(result)
            else:
                output_text = str(result)
            
            # Parse d·ªØ li·ªáu JSON tr·∫£ v·ªÅ t·ª´ LLM
            try:
                if isinstance(output_text, dict):
                    output_text = json.dumps(output_text, ensure_ascii=False)
                
                parsed = self.entity_chain.output_parser.parse(output_text)
                
                if isinstance(parsed, dict):
                    # N·∫øu parse th√†nh c√¥ng
                    entities = parsed.get("entities", {
                        "product_names": [],
                        "technical_terms": [],
                        "specifications": [],
                        "applications": [],
                        "features": [],
                        "issues": [],
                        "software_components": [],
                        "locations": []
                    })
                    confidence = parsed.get("confidence", 0.7)
                else:
                    # N·∫øu parse l·ªói, tr·∫£ k·∫øt qu·∫£ r·ªóng
                    entities = {k: [] for k in [
                        "product_names", "technical_terms", "specifications", "applications",
                        "features", "issues", "software_components", "locations"
                    ]}
                    confidence = 0.4
                    
            except Exception:
                # N·∫øu l·ªói parse ‚Üí fallback keyword
                entities = self._keyword_entity_extraction(question)
                confidence = 0.4
            
            # K·∫øt h·ª£p th√™m c√°c th·ª±c th·ªÉ ph√°t hi·ªán b·∫±ng keyword
            keyword_entities = self._keyword_entity_extraction(question)
            for key, value in keyword_entities.items():
                if value and key in entities:
                    existing_values = set(entities[key])
                    for v in value:
                        if v not in existing_values:
                            entities[key].append(v)
            
            # L∆∞u l·∫°i entities v√†o context memory
            if any(entities.values()):
                self.memory.last_entities = entities
            
            return {
                "entities": entities,
                "confidence": confidence,
                "raw_output": str(output_text)[:500] if output_text else ""
            }
            
        except Exception:
            return {
                "entities": self._keyword_entity_extraction(question),
                "confidence": 0.4,
                "raw_output": ""
            }

    # =======================================================
    # TR√çCH XU·∫§T ENTITY B·∫∞NG KEYWORDS (FALLBACK)
    # =======================================================
    def _keyword_entity_extraction(self, question: str) -> Dict[str, List[str]]:
        """
        Khi kh√¥ng c√≥ ph·∫£n h·ªìi t·ª´ AI ‚Üí t·ª± tr√≠ch xu·∫•t th·ª±c th·ªÉ b·∫±ng regex & t·ª´ kh√≥a.
        Ph√π h·ª£p v·ªõi Arbin: nh·∫≠n d·∫°ng th√¥ng s·ªë k·ªπ thu·∫≠t, s·∫£n ph·∫©m, ph·∫ßn m·ªÅm, l·ªói,...
        """
        question_lower = question.lower()
        question_no_accents = _strip_accents(question_lower)
        
        entities = {
            "product_names": [],
            "technical_terms": [],
            "specifications": [],
            "applications": [],
            "features": [],
            "issues": [],
            "software_components": [],
            "locations": []
        }
        
        # D√≤ pattern s·∫£n ph·∫©m
        product_patterns = [
            r'bt[-\s]?\d+', r'lbt[-\s]?\d*', r'mbt[-\s]?\d*', 
            r'mits\s*(?:pro|x)?', r'ev\s*(?:test|testing)?',
            r'battery\s+tester', r'cell\s+tester', r'battery\s+cycler'
        ]
        for pattern in product_patterns:
            matches = re.findall(pattern, question_lower, re.IGNORECASE)
            entities["product_names"].extend(matches)
        
        # D√≤ thu·∫≠t ng·ªØ k·ªπ thu·∫≠t
        for term in TECHNICAL_KEYWORDS:
            if term in question_lower or term in question_no_accents:
                entities["technical_terms"].append(term)
        
        # D√≤ th√¥ng s·ªë k·ªπ thu·∫≠t (d·∫°ng s·ªë + ƒë∆°n v·ªã)
        spec_patterns = [
            (r'(\d+(?:\.\d+)?)\s*(v|volts?)', 'voltage'),
            (r'(\d+(?:\.\d+)?)\s*(a|amps?)', 'current'),
            (r'(\d+(?:\.\d+)?)\s*(w|watts?)', 'power'),
            (r'(\d+(?:\.\d+)?)\s*(ah|mah)', 'capacity'),
            (r'(\d+(?:\.\d+)?)\s*%', 'accuracy'),
            (r'(\d+)\s*(channel|ch)s?', 'channels'),
            (r'¬±?\s*(\d+(?:\.\d+)?)\s*(?:v|a|w|%)', 'spec_value')
        ]
        for pattern, spec_type in spec_patterns:
            matches = re.findall(pattern, question_lower, re.IGNORECASE)
            for match in matches:
                value = match[0] if isinstance(match, tuple) else match
                entities["specifications"].append(f"{value} {spec_type}")
        
        # Ph·∫ßn m·ªÅm Arbin (Windaq, Console, MITS,...)
        software_keywords = ['windaq', 'mits', 'console', 'client', 'server', 'software', 'interface']
        for keyword in software_keywords:
            if keyword in question_lower:
                entities["software_components"].append(keyword)
        
        # ƒê·ªãa ƒëi·ªÉm ·ª©ng d·ª•ng (lab, factory,...)
        for location in LOCATION_TYPES:
            if location in question_lower:
                entities["locations"].append(location)
        
        # L·ªói k·ªπ thu·∫≠t ho·∫∑c s·ª± c·ªë
        issue_keywords = ['error', 'problem', 'issue', 'bug', 'fail', 'crash', 'not working', 'l·ªói']
        for keyword in issue_keywords:
            if keyword in question_lower:
                entities["issues"].append(keyword)
        
        return {k: v for k, v in entities.items() if v}

    # =======================================================
    # PIPELINE T·ªîNG H·ª¢P: process_nlu()
    # =======================================================
    def process_nlu(self, question: str, language: str = "en", session_id: str = "default") -> Dict[str, Any]:
        """
        G·ªçi to√†n b·ªô pipeline NLU g·ªìm:
        1. detect_intent() ‚Üí x√°c ƒë·ªãnh intent
        2. extract_entities() ‚Üí tr√≠ch xu·∫•t entities
        3. h·ª£p nh·∫•t k·∫øt qu·∫£ + th√™m context
        """
        logger.info(f"Processing NLU for question: '{question[:50]}...'")
        
        effective_language = language if language else "en"
        
        # Ch·∫°y tu·∫ßn t·ª± 2 b∆∞·ªõc
        intent_result = self.detect_intent(question, effective_language, session_id)
        entity_result = self.extract_entities(question, effective_language)

        # H·ª£p nh·∫•t k·∫øt qu·∫£
        merged = {
            "query": question,
            "language": language,
            "intent": intent_result["intent"],
            "intent_confidence": intent_result["confidence"],
            "entities": entity_result["entities"],
            "entity_confidence": entity_result["confidence"],
            "context": {
                "last_intent": intent_result.get("last_intent", ""),
                "last_question": intent_result.get("last_question", ""),
                "last_product_mentioned": intent_result.get("last_product_mentioned"),
                "memory_context": self.memory.get_context()
            },
            "enriched_text": intent_result.get("enriched_text"),
            "keywords_detected": intent_result.get("keywords_detected", {}),
            "raw_outputs": {
                "intent_raw": intent_result,
                "entity_raw": entity_result.get("raw_output", "")
            }
        }
        
        # T√≠nh ƒëi·ªÉm confidence t·ªïng th·ªÉ (weighted average)
        merged["overall_confidence"] = (
            intent_result["confidence"] * 0.6 + 
            entity_result["confidence"] * 0.4
        )
        
        # G·ª£i √Ω c√¢u h·ªèi ti·∫øp theo
        merged["suggested_responses"] = self._generate_suggested_responses(
            intent_result["intent"],
            entity_result["entities"]
        )
        
        logger.info(f"NLU Analysis complete: intent={merged['intent']}, confidence={merged['overall_confidence']:.2f}")
        return merged

    # =======================================================
    # T·∫†O C√ÇU G·ª¢I √ù (Suggested Responses)
    # =======================================================
    def _generate_suggested_responses(self, intent: str, entities: Dict[str, List[str]]) -> List[str]:
        """
        Sinh ra c√°c c√¢u h·ªèi g·ª£i √Ω cho ng∆∞·ªùi d√πng,
        t√πy theo intent hi·ªán t·∫°i v√† th√¥ng tin ƒë√£ ph√°t hi·ªán ƒë∆∞·ª£c.
        """
        suggestions = []
        
        # Intent: h·ªèi s·∫£n ph·∫©m
        if intent == "product_inquiry":
            if entities.get("product_names"):
                product = entities["product_names"][0]
                suggestions = [
                    f"What are the key specifications of {product}?",
                    f"What applications is {product} best suited for?",
                    f"How does {product} compare to similar models?",
                    f"What is the price range for {product}?"
                ]
            else:
                suggestions = [
                    "Which Arbin product are you interested in?",
                    "Are you looking for battery test systems or software?",
                    "What capacity range do you need for your testing?"
                ]
        
        # Intent: h·ªó tr·ª£ k·ªπ thu·∫≠t
        elif intent == "technical_support":
            suggestions = [
                "What specific error message are you seeing?",
                "Which software version are you currently using?",
                "Have you checked the troubleshooting guide in the manual?",
                "Is this a hardware or software issue?"
            ]
        
        # Intent: h·ªèi th√¥ng s·ªë k·ªπ thu·∫≠t
        elif intent == "specification_request":
            if entities.get("product_names"):
                product = entities["product_names"][0]
                suggestions = [
                    f"What is the voltage range of {product}?",
                    f"How many channels does {product} support?",
                    f"What is the measurement accuracy of {product}?"
                ]
            else:
                suggestions = [
                    "Which product specifications are you interested in?",
                    "Are you looking for voltage, current, or power specifications?",
                    "Do you need accuracy specifications or measurement ranges?"
                ]
        
        # Intent: so s√°nh s·∫£n ph·∫©m
        elif intent == "comparison_request":
            if len(entities.get("product_names", [])) >= 2:
                products = " and ".join(entities["product_names"][:2])
                suggestions = [f"What specific aspects of {products} would you like to compare?"]
            else:
                suggestions = [
                    "Which products would you like to compare?",
                    "Are you comparing different series or models?",
                    "What criteria are important for your comparison?"
                ]
        
        # Intent: h·ªèi gi√°
        elif intent == "pricing_inquiry":
            suggestions = [
                "Are you looking for academic or commercial pricing?",
                "Do you need a formal quote or just a price range?",
                "Would you like information about leasing options?"
            ]
        
        # Intent: h·ªèi ·ª©ng d·ª•ng
        elif intent == "application_info":
            suggestions = [
                "What type of batteries are you testing?",
                "Is this for research, quality control, or production?",
                "What is your testing throughput requirement?"
            ]
        
        return suggestions[:3]  # ch·ªâ l·∫•y 3 c√¢u ƒë·∫ßu ti√™n

    # =======================================================
    # BATCH ANALYZE (PH√ÇN T√çCH NHI·ªÄU C√ÇU)
    # =======================================================
    def batch_analyze(self, queries: List[str], language: str = "en") -> List[Dict[str, Any]]:
        """Ph√¢n t√≠ch h√†ng lo·∫°t c√¢u h·ªèi ‚Üí d√πng trong testing ho·∫∑c hu·∫•n luy·ªán."""
        results = []
        for query in queries:
            try:
                analysis = self.process_nlu(query, language)
                results.append(analysis)
            except Exception as e:
                logger.error(f"Error analyzing query '{query}': {e}")
                results.append({
                    "query": query,
                    "error": str(e),
                    "intent": "error",
                    "confidence": 0.0
                })
        return results


# =======================================================
# FACTORY FUNCTION: T·∫†O NLU PROCESSOR CHO ARBIN
# =======================================================
def create_nlu_processor(llm=None, memory_manager=None) -> NLUProcessor:
    """Factory function ti·ªán l·ª£i ƒë·ªÉ kh·ªüi t·∫°o NLUProcessor"""
    return NLUProcessor(llm=llm, memory_manager=memory_manager)

