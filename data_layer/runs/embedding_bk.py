"""
üß† STEP 3: T·∫°o embeddings v√† l∆∞u v√†o ChromaDB
T·ª± ƒë·ªông nh·∫≠n c·∫£ hai lo·∫°i d·ªØ li·ªáu:
    - data/processed/pdf/
    - data/processed/web/
"""

import os
import json
import glob
import logging
from data_layer.vector_store import EnhancedVectorStore
from data_layer.data_manager import DataManager

if __name__ == "__main__":
    # ===== Logging setup =====
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    print("üß† [3] Embedding and storing to ChromaDB...")

    # ===== Kh·ªüi t·∫°o c√°c ƒë·ªëi t∆∞·ª£ng c·∫ßn thi·∫øt =====
    data_manager = DataManager()
    vector_store = EnhancedVectorStore(
        persist_directory="./chroma_db",
        collection_name="arbin_documents"
    )

    # ===== T√¨m c√°c file processed JSON =====
    processed_dirs = ["./data/processed/pdf", "./data/processed/web"]
    processed_files = []

    for d in processed_dirs:
        if os.path.exists(d):
            processed_files.extend(glob.glob(os.path.join(d, "*.json")))

    if not processed_files:
        raise FileNotFoundError("‚ùå No processed files found in data/processed/. Please run step 3 first.")

    print(f"üìÇ Found {len(processed_files)} processed files to embed.\n")

    total_chunks = 0
    total_new = total_updated = total_duplicates = 0

    # ===== X·ª≠ l√Ω t·ª´ng file =====
    for processed_path in processed_files:
        file_name = os.path.basename(processed_path)
        subdir = "pdf" if "pdf" in processed_path.lower() else "web" if "web" in processed_path.lower() else "other"

        print(f"üìÑ Embedding {file_name} ‚Üí source: {subdir}")

        try:
            with open(processed_path, "r", encoding="utf-8") as f:
                chunks = json.load(f)
        except Exception as e:
            logging.error(f"‚ùå Failed to read {file_name}: {e}")
            continue

        if not isinstance(chunks, list) or not chunks:
            logging.warning(f"‚ö†Ô∏è Skipped {file_name}: invalid or empty data.")
            continue

        # ===== Th√™m v√†o vector store =====
        try:
            result = vector_store.add_document_chunks(chunks)
            total_chunks += len(chunks)
            total_new += result.get("new", 0)
            total_updated += result.get("updated", 0)
            total_duplicates += result.get("duplicates", 0)

            print(f"‚úÖ Embedded {len(chunks)} chunks ‚Üí {result['status']}")
            print(f"   New: {result.get('new', 0)}, Updated: {result.get('updated', 0)}, Duplicates: {result.get('duplicates', 0)}\n")

        except Exception as e:
            logging.error(f"‚ùå Error embedding {file_name}: {e}")

    # ===== Xu·∫•t th·ªëng k√™ t·ªïng =====
    print("üìä Embedding Summary:")
    print(f"   Total chunks processed: {total_chunks}")
    print(f"   New: {total_new}, Updated: {total_updated}, Duplicates skipped: {total_duplicates}")
    print(f"   Total in store: {vector_store.collection.count()}")
    print(f"   Embedding model: {vector_store.embedding_model_name}")

    # ===== Xu·∫•t th·ªëng k√™ ra file inspection =====
    stats = {
        "total_chunks": total_chunks,
        "new": total_new,
        "updated": total_updated,
        "duplicates": total_duplicates,
        "collection_count": vector_store.collection.count(),
        "model": vector_store.embedding_model_name,
    }
    data_manager.export_for_inspection(stats, "embedding_stats", "json")

    print("\n‚úÖ Embedding complete. Check ChromaDB folder: ./chroma_db/")
