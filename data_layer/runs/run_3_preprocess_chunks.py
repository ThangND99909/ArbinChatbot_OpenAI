"""
üßπ STEP 2: L√†m s·∫°ch v√† chia nh·ªè (chunk) d·ªØ li·ªáu ƒë√£ thu th·∫≠p
‚úÖ T·ª± ƒë·ªông x·ª≠ l√Ω c·∫£ hai ngu·ªìn:
    - PDF / DOCX (enhanced_document_processor_raw_*.json)
    - Web (web_raw_*.json)
D·ªØ li·ªáu sau khi chunk ƒë∆∞·ª£c l∆∞u v√†o:
    - data/processed/pdf/
    - data/processed/web/
"""

import os
import json
import glob
import logging
from data_layer.data_manager import DataManager
from data_layer.preprocessor import TextPreprocessor

if __name__ == "__main__":
    # ===== Logging setup =====
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s"
    )
    print("üßπ [2] Cleaning and chunking all raw data sources...")

    # ===== Kh·ªüi t·∫°o DataManager v√† Preprocessor =====
    data_manager = DataManager()
    preprocessor = TextPreprocessor(chunk_size=800, chunk_overlap=150)

    # ===== Qu√©t th∆∞ m·ª•c data/raw/ ƒë·ªÉ t√¨m file raw =====
    raw_dir = "./data/raw"
    raw_files = sorted(
        [os.path.basename(f) 
         for f in glob.glob(os.path.join(raw_dir, "*.json")) 
         if "raw" in f]
    )

    if not raw_files:
        raise FileNotFoundError("‚ùå No raw files found in ./data/raw/. Please run step 1 first.")

    total_chunks = 0
    summary = []

    print(f"üìÇ Found {len(raw_files)} raw files to process.\n")

    # ===== X·ª≠ l√Ω t·ª´ng file raw =====
    for raw_file in raw_files:
        raw_path = os.path.join(raw_dir, raw_file)

        # X√°c ƒë·ªãnh lo·∫°i d·ªØ li·ªáu (pdf/documents ho·∫∑c web)
        if "enhanced_document_processor_raw" in raw_file.lower():
            subdir = "pdf"
        elif "web" in raw_file.lower():
            subdir = "web"
        else:
            subdir = "other"  # N·∫øu mu·ªën, c√≥ th·ªÉ skip nh·ªØng file kh√°c

        print(f"üìÑ Processing raw file: {raw_file} ‚Üí subdir: {subdir}")

        # ƒê·ªçc JSON
        try:
            with open(raw_path, "r", encoding="utf-8") as f:
                raw_docs = json.load(f)
        except Exception as e:
            logging.error(f"‚ùå Failed to load {raw_file}: {e}")
            continue

        # B·ªè qua n·∫øu file r·ªóng ho·∫∑c kh√¥ng h·ª£p l·ªá
        if not isinstance(raw_docs, list) or not raw_docs:
            logging.warning(f"‚ö†Ô∏è Skipped {raw_file}: empty or invalid structure.")
            continue

        # L√†m s·∫°ch v√† chia nh·ªè
        try:
            chunks = preprocessor.clean_and_chunk(raw_docs)
        except Exception as e:
            logging.error(f"‚ùå Error cleaning/chunking {raw_file}: {e}")
            continue

        total_chunks += len(chunks)

        # L∆∞u l·∫°i
        try:
            processor_type = "enhanced" if subdir == "pdf" else "web"
            data_manager.save_processed_data(chunks, processor_type=processor_type, subdir=subdir)
            summary.append({"source": subdir, "file": raw_file, "chunks": len(chunks)})
            print(f"‚úÖ Processed {len(chunks)} chunks for {subdir} ({raw_file})\n")
        except Exception as e:
            logging.error(f"‚ùå Failed to save processed data for {raw_file}: {e}")

    # ===== T·ªïng k·∫øt =====
    print("\nüìä Summary:")
    if summary:
        for s in summary:
            print(f" - {s['file']} ‚Üí {s['chunks']} chunks ({s['source']})")
        print(f"\nüéØ Done! Total chunks created: {total_chunks}")
        print("üëâ Check output folders:")
        print("   - data/processed/pdf/")
        print("   - data/processed/web/")
    else:
        print("‚ö†Ô∏è No valid raw files were processed.")
