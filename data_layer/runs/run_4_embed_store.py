"""
üß† STEP 4: T·∫°o embeddings v√† l∆∞u v√†o ChromaDB v·ªõi SentenceTransformer (LOCAL)
"""

import os
import json
import glob
import logging
from tqdm import tqdm
from data_layer.vector_store import EnhancedVectorStore
from data_layer.data_manager import DataManager
import sys
import uuid

# TH√äM: Fix encoding cho Windows console
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# ===== MONKEY PATCH: DISABLE DEDUPLICATION =====
print("‚ö†Ô∏è APPLYING MONKEY PATCH TO DISABLE DEDUPLICATION")

import data_layer.vector_store as vs_module
import numpy as np

# Store original for reference
original_add_documents = vs_module.EnhancedVectorStore.add_documents

def patched_add_documents(self, chunks, batch_size=100, update_existing=True):
    """
    Simplified add_documents without duplicate checking
    - Converts numpy arrays to lists properly
    - Adds all documents without deduplication
    """
    try:
        # Prepare all documents
        all_documents = []
        all_metadatas = []
        all_ids = []
        
        print(f"   Processing {len(chunks)} chunks...")
        
        for i, chunk in enumerate(chunks):
            text = chunk.get('text', '').strip()
            metadata = chunk.get('metadata', {})
            metadata = self._validate_metadata(metadata)
            
            # Skip empty or very short text
            if not text or len(text) < 20:
                continue
            
            # Generate unique ID
            doc_id = f"doc_{uuid.uuid4().hex}"
            
            all_documents.append(text)
            all_metadatas.append(metadata)
            all_ids.append(doc_id)
            
            # Progress update for large files
            if i > 0 and i % 1000 == 0:
                print(f"     Prepared {i}/{len(chunks)} chunks...")
        
        if not all_documents:
            print("   No valid documents to add")
            return {'status': 'no_changes', 'new': 0, 'updated': 0, 'duplicates': 0}
        
        print(f"   Prepared {len(all_documents)} valid documents for embedding")
        
        # Process in smaller batches to avoid memory issues
        total_added = 0
        small_batch_size = min(50, batch_size)  # Smaller batches for local model
        batch_count = (len(all_documents) + small_batch_size - 1) // small_batch_size
        
        print(f"   Processing in {batch_count} batches of {small_batch_size}...")
        
        for batch_num in range(batch_count):
            start_idx = batch_num * small_batch_size
            end_idx = start_idx + small_batch_size
            
            batch_docs = all_documents[start_idx:end_idx]
            batch_metas = all_metadatas[start_idx:end_idx]
            batch_ids = all_ids[start_idx:end_idx]
            
            if not batch_docs:
                continue
            
            print(f"   Batch {batch_num+1}/{batch_count}: {len(batch_docs)} documents")
            
            try:
                # Create embeddings
                embeddings = self.create_embeddings(batch_docs)
                
                # DEBUG: Check embeddings type
                if embeddings is None:
                    print(f"     ‚ö†Ô∏è No embeddings generated, skipping batch")
                    continue
                
                # Convert numpy arrays to lists
                if hasattr(embeddings, 'tolist'):
                    embeddings = embeddings.tolist()
                elif isinstance(embeddings, np.ndarray):
                    embeddings = embeddings.tolist()
                
                # Ensure embeddings is a list of lists
                if embeddings and not isinstance(embeddings[0], list):
                    embeddings = [embeddings] if isinstance(embeddings, list) else [[embeddings]]
                
                # Add to collection
                self.collection.add(
                    embeddings=embeddings,
                    documents=batch_docs,
                    metadatas=batch_metas,
                    ids=batch_ids
                )
                
                total_added += len(batch_docs)
                print(f"     ‚úì Added {len(batch_docs)} documents")
                
            except Exception as e:
                error_msg = str(e)
                print(f"     ‚úó Batch error: {error_msg[:100]}")
                
                # Try one document at a time as fallback
                successful_in_batch = 0
                for j in range(len(batch_docs)):
                    try:
                        # Get single embedding
                        single_embedding = self.create_embeddings([batch_docs[j]])
                        
                        if single_embedding is None:
                            print(f"       ‚ö†Ô∏è No embedding for document {j+1}, skipping")
                            continue
                        
                        # Convert to list
                        if hasattr(single_embedding, 'tolist'):
                            single_embedding = single_embedding.tolist()
                        elif isinstance(single_embedding, np.ndarray):
                            single_embedding = single_embedding.tolist()
                        
                        # Ensure proper format
                        if isinstance(single_embedding, list) and len(single_embedding) > 0:
                            if not isinstance(single_embedding[0], list):
                                single_embedding = [single_embedding]
                        
                        self.collection.add(
                            embeddings=single_embedding,
                            documents=[batch_docs[j]],
                            metadatas=[batch_metas[j]],
                            ids=[batch_ids[j]]
                        )
                        total_added += 1
                        successful_in_batch += 1
                        
                    except Exception as e2:
                        error_msg2 = str(e2)
                        if "ambiguous" in error_msg2:
                            print(f"       ‚úó Skipping document {j+1}: numpy array issue")
                        else:
                            print(f"       ‚úó Failed document {j+1}: {error_msg2[:80]}")
                
                if successful_in_batch > 0:
                    print(f"     ‚û§ Successfully added {successful_in_batch}/{len(batch_docs)} documents from failed batch")
        
        print(f"   Total documents added: {total_added}")
        
        # Update metrics if they exist
        if hasattr(self, 'metrics'):
            self.metrics['total_added'] = total_added
            self.metrics['collection_size'] = self.collection.count()
        
        return {
            'status': 'success' if total_added > 0 else 'partial',
            'new': total_added,
            'updated': 0,
            'duplicates': 0,
            'collection_size': self.collection.count(),
            'total_processed': len(all_documents)
        }
        
    except Exception as e:
        import traceback
        print(f"‚ùå Critical error in patched_add_documents: {e}")
        traceback.print_exc()
        return {'status': 'error', 'error': str(e)}

# Apply the patch
vs_module.EnhancedVectorStore.add_documents = patched_add_documents
print("‚úÖ Deduplication disabled via monkey patch")
print("="*50 + "\n")

# ===== MAIN SCRIPT =====

if __name__ == "__main__":
    # ===== Logging setup =====
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler("embedding.log", encoding="utf-8"),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    print("üß† [4] Embedding and storing to ChromaDB with SentenceTransformer (LOCAL)...")
    
    # ===== Kh·ªüi t·∫°o c√°c ƒë·ªëi t∆∞·ª£ng c·∫ßn thi·∫øt =====
    data_manager = DataManager()
    
    # S·ª≠ d·ª•ng SentenceTransformer LOCAL embeddings - KH√îNG C·∫¶N API KEY
    vector_store = EnhancedVectorStore(
        persist_directory="./chroma_db",
        collection_name="arbin_documents",
        embedding_model="intfloat/multilingual-e5-base",  # Model local h·ªó tr·ª£ ti·∫øng Vi·ªát
        embedding_batch_size=256,  # Batch nh·ªè h∆°n cho local model
        max_collection_size=200000,
        enable_backup=True
        # KH√îNG C·∫¶N openai_api_key n·ªØa
    )
    
    # ===== T√¨m c√°c file processed JSON =====
    processed_dirs = ["./data/processed/pdf", "./data/processed/web"]
    processed_files = []
    
    for d in processed_dirs:
        if os.path.exists(d):
            processed_files.extend(glob.glob(os.path.join(d, "*.json")))
    
    if not processed_files:
        raise FileNotFoundError("‚ùå No processed files found. Please run step 3 first.")
    
    print(f"üìÇ Found {len(processed_files)} processed files to embed.\n")
    
    total_chunks = 0
    total_new = total_updated = total_duplicates = 0
    
    # ===== X·ª≠ l√Ω t·ª´ng file v·ªõi progress bar =====
    for processed_path in tqdm(processed_files, desc="Processing files"):
        file_name = os.path.basename(processed_path)
        subdir = "pdf" if "pdf" in processed_path.lower() else "web" if "web" in processed_path.lower() else "other"
        
        print(f"\nüìÑ Embedding {file_name} ‚Üí source: {subdir}")
        
        try:
            with open(processed_path, "r", encoding="utf-8") as f:
                chunks = json.load(f)
        except Exception as e:
            logging.error(f"‚ùå Failed to read {file_name}: {e}")
            continue
        
        if not isinstance(chunks, list) or not chunks:
            logging.warning(f"‚ö†Ô∏è Skipped {file_name}: invalid or empty data.")
            continue
        
        total_chunks += len(chunks)
        
        # ===== Th√™m v√†o vector store =====
        try:
            result = vector_store.add_document_chunks(
                chunks,
                batch_size=100,  # Gi·∫£m batch size cho local processing
                update_existing=False
            )
            
            total_new += result.get("new", 0)
            total_updated += result.get("updated", 0)
            total_duplicates += result.get("duplicates", 0)
            
            print(f"‚úÖ Embedded {len(chunks)} chunks ‚Üí {result['status']}")
            print(f"   New: {result.get('new', 0)}, Updated: {result.get('updated', 0)}, Duplicates: {result.get('duplicates', 0)}")
            
            # Show collection size after each file
            current_size = vector_store.collection.count()
            print(f"   Collection size: {current_size} documents")
            
        except Exception as e:
            logging.error(f"‚ùå Error embedding {file_name}: {e}")
            import traceback
            traceback.print_exc()
    
    # ===== Xu·∫•t th·ªëng k√™ t·ªïng =====
    print("\n" + "="*50)
    print("üìä Embedding Summary:")
    print("="*50)
    print(f"   Total chunks processed: {total_chunks}")
    print(f"   New documents: {total_new}")
    print(f"   Updated documents: {total_updated}")
    print(f"   Duplicates skipped: {total_duplicates}")
    
    final_count = vector_store.collection.count()
    print(f"   Total in store: {final_count}")
    
    if final_count < total_new:
        print(f"   ‚ö†Ô∏è Warning: Collection has {final_count} docs but {total_new} were reported as new")
    
    print(f"   Embedding model: {vector_store.embedding_model_name}")
    print(f"   Embedding dimension: {vector_store.embedding_dimension}")
    
    # L·∫•y th·ªëng k√™ local embedding
    stats = vector_store.get_collection_stats()
    print(f"   Embedding time: {stats.get('embedding_time_seconds', 0):.2f}s")
    print(f"   Embedding speed: {stats.get('embedding_speed', 'N/A')}")
    
    # ===== Xu·∫•t th·ªëng k√™ ra file inspection =====
    export_stats = {
        "total_chunks": total_chunks,
        "new": total_new,
        "updated": total_updated,
        "duplicates": total_duplicates,
        "collection_count": final_count,
        "model": vector_store.embedding_model_name,
        "embedding_dimension": vector_store.embedding_dimension,
        "embedding_time_seconds": stats.get('embedding_time_seconds', 0),
        "embedding_speed": stats.get('embedding_speed', 'N/A'),
        "processed_files": len(processed_files),
        "status": "success" if final_count > 100 else "warning_low_docs"
    }
    
    data_manager.export_for_inspection(export_stats, "embedding_stats", "json")
    
    print("\n" + "="*50)
    if final_count > 1000:
        print("‚úÖ‚úÖ‚úÖ Embedding complete with SUCCESS!")
        print(f"   Vector store now has {final_count} documents")
    elif final_count > 100:
        print("‚úÖ Embedding complete!")
        print(f"   Vector store has {final_count} documents")
    else:
        print("‚ö†Ô∏è Embedding complete but LOW DOCUMENT COUNT!")
        print(f"   Vector store only has {final_count} documents")
        print("   Check deduplication logic and chunk quality")
    
    print(f"‚úÖ Check ChromaDB folder: ./chroma_db/")
    print(f"‚úÖ Stats saved to: ./data/inspection/embedding_stats.json")

    # ===== KI·ªÇM TRA VECTOR STORE SAU KHI EMBEDDING =====
    print("\n" + "="*60)
    print("üîç KI·ªÇM TRA VECTOR STORE SAU KHI EMBEDDING")
    print("="*60)

    # 1Ô∏è‚É£ L·∫•y th·ªëng k√™ nhanh
    stats = vector_store.get_collection_stats()
    print(f"üì¶ T·ªïng s·ªë documents: {stats.get('total_documents', 0)}")
    print(f"üìà Model: {stats.get('embedding_model', 'unknown')}")
    print(f"üìè Dimension: {stats.get('embedding_dimension', 0)}")
    print(f"üíæ L∆∞u t·∫°i: {stats.get('persist_directory', './chroma_db')}")
    print(f"üöÄ T·ªëc ƒë·ªô embedding trung b√¨nh: {stats.get('embedding_speed', 'N/A')}")

    # 2Ô∏è‚É£ Xem 3 document ƒë·∫ßu ti√™n
    print("\nüìÑ Xem tr∆∞·ªõc 3 documents ƒë·∫ßu ti√™n trong vector store:\n")
    try:
        results = vector_store.collection.peek(limit=3)
        for i in range(len(results['ids'])):
            doc_id = results['ids'][i]
            text = results['documents'][i][:250].replace("\n", " ") + "..."
            metadata = results['metadatas'][i]
            print(f"--- Document {i+1} ---")
            print(f"ID: {doc_id}")
            print(f"Source: {metadata.get('source', 'unknown')}")
            print(f"Type: {metadata.get('source_type', 'unknown')}")
            print(f"Text: {text}\n")
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ xem tr∆∞·ªõc document: {e}")

    # 3Ô∏è‚É£ Th·ª≠ search v√†i truy v·∫•n ki·ªÉm ch·ª©ng
    print("="*60)
    print("üîé TH·ª¨ SEARCH KI·ªÇM TRA T√çNH LI√äN QUAN")
    print("="*60)

    test_queries = [
        "ch√≠nh s√°ch b·∫£o h√†nh c·ªßa Arbin",
        "Arbin BT-2000 specifications",
        "l·ªói calibration trong ph·∫ßn m·ªÅm MITS Pro"
    ]

    for query in test_queries:
        print(f"\nüß© Truy v·∫•n: {query}")
        try:
            results = vector_store.search_similar(query, k=3)
            if not results:
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£.")
                continue
            for idx, r in enumerate(results):
                text = r['text'][:180].replace("\n", " ")
                score = r['score']
                src = r['metadata'].get('source', 'unknown')
                print(f"  {idx+1}. [{score:.2f}] {text}  (Source: {src})")
        except Exception as e:
            print(f"‚ùå L·ªói khi search: {e}")

    print("\n‚úÖ Ki·ªÉm tra vector store ho√†n t·∫•t!")
    print("B·∫°n c√≥ th·ªÉ ch·∫°y truy v·∫•n th·ª≠ b·∫±ng chatbot ngay b√¢y gi·ªù üéØ")
    print("="*60)