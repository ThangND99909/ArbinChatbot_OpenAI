#Crawl toÃ n bá»™ ná»™i dung trang https://www.arbin.com/
import logging
import json
from data_layer.data_manager import DataManager
from data_layer.web_crawler import EnhancedWebCrawler  # Ä‘áº£m báº£o file web_crawler.py cá»§a báº¡n náº±m cÃ¹ng data_layer/

if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s"
    )

    print("ğŸŒ [5] Starting web ingestion from Arbin website...")

    # 1ï¸âƒ£ Khá»Ÿi táº¡o DataManager vÃ  WebCrawler
    data_manager = DataManager()
    crawler = EnhancedWebCrawler(base_url="https://www.arbin.com/")

    # 2ï¸âƒ£ Crawl website (incremental hoáº·c force full)
    force_full = False  # Ä‘á»•i True náº¿u báº¡n muá»‘n crawl láº¡i toÃ n bá»™
    documents = crawler.crawl_site(force_recrawl=force_full)

    # 3ï¸âƒ£ Kiá»ƒm tra káº¿t quáº£
    print(f"\nğŸ“‹ Káº¾T QUáº¢ Tá»”NG Há»¢P:")
    print(f"âœ… Crawled {len(documents)} web pages thÃ nh cÃ´ng.")
    
    # Hiá»ƒn thá»‹ thá»‘ng kÃª tá»« crawler
    stats = crawler.get_statistics_summary()
    print(f"ğŸ“Š Tá»· lá»‡ thÃ nh cÃ´ng: {stats['success_rate']:.1f}%")
    print(f"ğŸ“Š Tá»· lá»‡ tháº¥t báº¡i: {stats['failure_rate']:.1f}%")
    
    if stats['failed_urls']:
        failed_path = "./data/inspection/failed_urls_arbin.json"
        with open(failed_path, "w", encoding="utf-8") as f:
            json.dump(stats['failed_urls'], f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ Danh sÃ¡ch URL tháº¥t báº¡i Ä‘Ã£ lÆ°u: {failed_path}")
        for i, failed in enumerate(stats['failed_urls'][:5], 1):
            print(f"  {i}. {failed['url']}")
            print(f"     Lá»—i: {failed['error'][:80]}...")

    # LÆ°u skipped URLs
    if crawler.skipped_urls:
        skipped_path = "./data/inspection/skipped_urls_arbin.json"
        with open(skipped_path, "w", encoding="utf-8") as f:
            json.dump(crawler.skipped_urls, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ Skipped URLs saved to {skipped_path}")
    
    if not documents:
        print("âš ï¸ No new documents found â€” site may be up-to-date.")
    else:
        # 4ï¸âƒ£ LÆ°u dá»¯ liá»‡u raw
        data_manager.save_raw_data(documents, "web_arbin")
        data_manager.save_document_metadata(documents, "web_arbin")

        # 5ï¸âƒ£ LÆ°u thá»‘ng kÃª cÆ¡ báº£n
        stats = {
            "total_pages": len(documents),
            "timestamp": documents[0].get("crawled_at") if documents else None,
            "sample_urls": [d["url"] for d in documents[:5]],
            "source": "arbin.com",
        }

        data_manager.export_for_inspection(stats, "web_arbin_stats", "json")
        print(json.dumps(stats, indent=2, ensure_ascii=False))

        

    print("ğŸ“‚ Web crawl data saved in ./data/raw/ and ./data/inspection/")
