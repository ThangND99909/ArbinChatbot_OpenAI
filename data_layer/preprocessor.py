from typing import List, Dict, Any, Optional, Tuple
import re
import hashlib
import logging
from datetime import datetime

logger = logging.getLogger(__name__)


class TextPreprocessor:
    """
    TextPreprocessor: Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n th√¥ng minh
    T·∫≠p trung v√†o:
    1. L√†m s·∫°ch vƒÉn b·∫£n (clean_text) - GI·ªÆ th√¥ng tin quan tr·ªçng
    2. Chia nh·ªè vƒÉn b·∫£n th√†nh c√°c chunk v·ªõi boundary-aware
    3. Th√™m metadata markers cho c√°c lo·∫°i th√¥ng tin ƒë·∫∑c bi·ªát
    """

    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        Kh·ªüi t·∫°o preprocessor v·ªõi c·∫•u h√¨nh chunk

        Args:
            chunk_size: S·ªë t·ª´ t·ªëi ƒëa trong m·ªói chunk (m·∫∑c ƒë·ªãnh: 1000)
            chunk_overlap: S·ªë t·ª´ overlap gi·ªØa c√°c chunk li·ªÅn k·ªÅ (m·∫∑c ƒë·ªãnh: 200)
        
        Raises:
            ValueError: N·∫øu tham s·ªë kh√¥ng h·ª£p l·ªá
        """
        # Validate parameters
        if chunk_size <= 0:
            raise ValueError("chunk_size must be positive")
        if chunk_overlap < 0:
            raise ValueError("chunk_overlap cannot be negative")
        if chunk_overlap >= chunk_size:
            raise ValueError("chunk_overlap must be smaller than chunk_size")
        
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # Pre-compile regex patterns for performance
        self._compile_patterns()
        
        logger.info(f"Initialized TextPreprocessor with chunk_size={chunk_size}, chunk_overlap={chunk_overlap}")
    
    def _compile_patterns(self):
        """Compile t·∫•t c·∫£ regex patterns m·ªôt l·∫ßn ƒë·ªÉ tƒÉng hi·ªáu nƒÉng"""
        # HTML cleaning patterns
        self.script_style_pattern = re.compile(
            r'<(script|style).*?>.*?</\1>',
            flags=re.DOTALL | re.IGNORECASE
        )
        self.html_tags_pattern = re.compile(r'<[^>]+>')
        
        # Whitespace normalization
        self.whitespace_pattern = re.compile(r'[ \t]+')
        self.multiple_newlines_pattern = re.compile(r'\n\s*\n\s*\n+')
        
        # Contact info patterns
        self.phone_pattern = re.compile(
            r'(\+\d{1,3})[\s\-\.]*(\d{1,})[\s\-\.]*(\d{1,})[\s\-\.]*(\d{1,})'
        )
        self.email_pattern = re.compile(
            r'([a-zA-Z0-9._%+-]+)\s*@\s*([a-zA-Z0-9.-]+)\s*\.\s*([a-zA-Z]{2,})'
        )
        
        # Metadata markers - ORDERED by priority (specific to general)
        self.metadata_patterns = [
            # Contact types (most specific first)
            (re.compile(r'\b(Sales Department|Sales Team|B√°n h√†ng|Ph√≤ng kinh doanh)\b', re.IGNORECASE), 'DEPT_SALES'),
            (re.compile(r'\b(Support Department|Technical Support|H·ªó tr·ª£ k·ªπ thu·∫≠t)\b', re.IGNORECASE), 'DEPT_SUPPORT'),
            (re.compile(r'\b(Engineering Department|K·ªπ thu·∫≠t)\b', re.IGNORECASE), 'DEPT_ENGINEERING'),
            (re.compile(r'\b(Marketing Department|Marketing|Ti·∫øp th·ªã)\b', re.IGNORECASE), 'DEPT_MARKETING'),
            
            # Special email types
            (re.compile(r'(sales[\.\-]?[a-z]*@|support[\.\-]?[a-z]*@)', re.IGNORECASE), 'CONTACT_TYPE'),
            
            # Contact info
            (re.compile(r'([\w\.\-]+@[\w\.\-]+\.[\w]+)', re.IGNORECASE), 'EMAIL'),
            (re.compile(r'(\+\d[\d\s\-\(\)\.]{7,})'), 'PHONE'),
            
            # People
            (re.compile(r'\b(Mr\.|Ms\.|Mrs\.|Dr\.|Professor|Prof\.)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b'), 'PERSON'),
            
            # Countries
            (re.compile(r'\b(USA|U\.S\.A\.|United States)\b', re.IGNORECASE), 'COUNTRY_USA'),
            (re.compile(r'\b(Germany|Deutschland)\b', re.IGNORECASE), 'COUNTRY_GERMANY'),
            (re.compile(r'\b(China|‰∏≠ÂõΩ|Beijing|Âåó‰∫¨)\b', re.IGNORECASE), 'COUNTRY_CHINA'),
            (re.compile(r'\b(India|‡§≠‡§æ‡§∞‡§§|Pune|‡§Æ‡•Å‡§Ç‡§¨‡§à)\b', re.IGNORECASE), 'COUNTRY_INDIA'),
            (re.compile(r'\b(Vietnam|Vi·ªát Nam|Hanoi|H·ªì Ch√≠ Minh)\b', re.IGNORECASE), 'COUNTRY_VIETNAM'),
            (re.compile(r'\b(Taiwan|Ëá∫ÁÅ£|Âè∞Âåó)\b', re.IGNORECASE), 'COUNTRY_TAIWAN'),
            (re.compile(r'\b(Korea|ÌïúÍµ≠|Seoul|ÏÑúÏö∏)\b', re.IGNORECASE), 'COUNTRY_KOREA'),
            
            # Regions
            (re.compile(r'\b(North America|Northern America)\b', re.IGNORECASE), 'REGION_NA'),
            (re.compile(r'\b(South America|Latin America)\b', re.IGNORECASE), 'REGION_SA'),
            (re.compile(r'\b(Europe|European Union|EU)\b', re.IGNORECASE), 'REGION_EU'),
            (re.compile(r'\b(Middle East|‰∏≠‰∏ú|ÿßŸÑÿ¥ÿ±ŸÇ ÿßŸÑÿ£Ÿàÿ≥ÿ∑)\b', re.IGNORECASE), 'REGION_ME'),
            (re.compile(r'\b(Africa|ÈùûÊ¥≤|ÿ£ŸÅÿ±ŸäŸÇŸäÿß)\b', re.IGNORECASE), 'REGION_AFRICA'),
            (re.compile(r'\b(Southeast Asia|SE Asia|‰∏úÂçó‰∫ö|ƒê√¥ng Nam √Å)\b', re.IGNORECASE), 'REGION_SEA'),
            (re.compile(r'\b(Oceania|Australia|Australasia)\b', re.IGNORECASE), 'REGION_OCEANIA'),
            (re.compile(r'\b(Asia Pacific|APAC)\b', re.IGNORECASE), 'REGION_APAC'),
        ]
    
    def clean_text(self, text: str) -> str:
        """
        L√†m s·∫°ch vƒÉn b·∫£n NH∆ØNG GI·ªÆ TH√îNG TIN QUAN TR·ªåNG
        
        Args:
            text: VƒÉn b·∫£n ƒë·∫ßu v√†o c√≥ th·ªÉ ch·ª©a HTML, ƒë·ªãnh d·∫°ng k·ª≥ l·∫°
            
        Returns:
            VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v·ªõi metadata markers
        """
        if not text or not isinstance(text, str):
            return ""
        
        original_length = len(text)
        
        # 1Ô∏è‚É£ X√ìA HTML TAGS NH∆ØNG GI·ªÆ N·ªòI DUNG
        text = self.script_style_pattern.sub('', text)
        text = self.html_tags_pattern.sub(' ', text)
        
        # 2Ô∏è‚É£ CHU·∫®N H√ìA KHO·∫¢NG TR·∫ÆNG NH∆ØNG GI·ªÆ C·∫§U TR√öC
        text = self.whitespace_pattern.sub(' ', text)
        text = self.multiple_newlines_pattern.sub('\n\n', text)
        
        # 3Ô∏è‚É£ CHU·∫®N H√ìA TH√îNG TIN CONTACT
        text = self.phone_pattern.sub(r'\1 \2 \3 \4', text)
        text = self.email_pattern.sub(r'\1@\2.\3', text)
        
        # 4Ô∏è‚É£ TH√äM METADATA MARKERS - S·ª¨ D·ª§NG SINGLE PASS
        # T√¨m t·∫•t c·∫£ matches v√† s·∫Øp x·∫øp theo priority
        replacements = []
        
        for pattern, marker in self.metadata_patterns:
            for match in pattern.finditer(text):
                start, end = match.span()
                match_text = match.group()
                
                # Ki·ªÉm tra overlap v·ªõi c√°c replacements ƒë√£ t√¨m th·∫•y
                overlap = False
                for (rep_start, rep_end, _) in replacements:
                    if not (end <= rep_start or start >= rep_end):
                        overlap = True
                        break
                
                if not overlap:
                    replacements.append((start, end, f"[{marker}: {match_text}]"))
        
        # Apply replacements t·ª´ cu·ªëi l√™n ƒë·∫ßu ƒë·ªÉ gi·ªØ nguy√™n index
        for start, end, replacement in sorted(replacements, reverse=True):
            text = text[:start] + replacement + text[end:]
        
        # 5Ô∏è‚É£ FINAL CLEANUP
        text = text.strip()
        
        # Log compression ratio
        if original_length > 0:
            compression_ratio = (1 - len(text) / original_length) * 100
            logger.debug(f"Cleaned text: {original_length:,} ‚Üí {len(text):,} chars ({compression_ratio:.1f}% reduction)")
        
        return text
    
    def _find_chunk_boundary(self, words: List[str], start_idx: int, max_end_idx: int) -> int:
        """
        T√¨m ƒëi·ªÉm k·∫øt th√∫c chunk t·ªët nh·∫•t, kh√¥ng c·∫Øt gi·ªØa c√¢u ho·∫∑c marker
        
        Args:
            words: Danh s√°ch t·ª´
            start_idx: V·ªã tr√≠ b·∫Øt ƒë·∫ßu
            max_end_idx: V·ªã tr√≠ k·∫øt th√∫c t·ªëi ƒëa (theo chunk_size)
            
        Returns:
            V·ªã tr√≠ k·∫øt th√∫c th√≠ch h·ª£p
        """
        if max_end_idx >= len(words):
            return len(words)
        
        # ∆Øu ti√™n 1: Kh√¥ng c·∫Øt gi·ªØa metadata marker [...]
        if '[' in words[max_end_idx - 1] and ']' not in words[max_end_idx - 1]:
            # T√¨m d·∫•u ƒë√≥ng marker ]
            for i in range(max_end_idx, min(max_end_idx + 10, len(words))):
                if ']' in words[i]:
                    return i + 1
            return max_end_idx
        
        # ∆Øu ti√™n 2: K·∫øt th√∫c ·ªü d·∫•u c√¢u
        sentence_enders = {'.', '!', '?', '„ÄÇ', 'ÔºÅ', 'Ôºü'}
        for i in range(min(10, max_end_idx - start_idx)):
            check_idx = max_end_idx - i - 1
            if check_idx >= start_idx:
                word = words[check_idx]
                if any(word.endswith(ender) for ender in sentence_enders):
                    return check_idx + 1
        
        # ∆Øu ti√™n 3: K·∫øt th√∫c ·ªü d·∫•u ph·∫©y, ch·∫•m ph·∫©y
        comma_enders = {',', ';', 'Ôºå', 'Ôºõ'}
        for i in range(min(5, max_end_idx - start_idx)):
            check_idx = max_end_idx - i - 1
            if check_idx >= start_idx:
                word = words[check_idx]
                if any(word.endswith(ender) for ender in comma_enders):
                    return check_idx + 1
        
        # Default: return max_end_idx
        return max_end_idx
    
    def split_into_chunks(self, text: str, metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        Chia vƒÉn b·∫£n th√†nh c√°c chunk v·ªõi boundary-aware splitting
        - Kh√¥ng c·∫Øt gi·ªØa c√¢u ho·∫∑c metadata markers
        - T·∫°o metadata chi ti·∫øt cho m·ªói chunk

        Args:
            text: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c clean (n√™n clean tr∆∞·ªõc)
            metadata: Metadata g·ªëc c·ªßa document

        Returns:
            List c√°c dict chunk, m·ªói chunk c√≥ text + metadata
        """
        if not text:
            return []
        
        metadata = metadata or {}
        
        # Split theo t·ª´
        words = text.split()
        if not words:
            return []
        
        chunks = []
        doc_hash = hashlib.md5(text.encode()).hexdigest()[:8]
        
        start_idx = 0
        chunk_num = 0
        
        while start_idx < len(words):
            # X√°c ƒë·ªãnh end index c∆° b·∫£n
            basic_end_idx = min(start_idx + self.chunk_size, len(words))
            
            # T√¨m boundary t·ªët h∆°n
            end_idx = self._find_chunk_boundary(words, start_idx, basic_end_idx)
            
            # ƒê·∫£m b·∫£o c√≥ overlap t·ªëi thi·ªÉu
            if chunk_num > 0 and end_idx - start_idx < self.chunk_size // 4:
                end_idx = min(start_idx + self.chunk_size // 2, len(words))
            
            chunk_words = words[start_idx:end_idx]
            chunk_text = ' '.join(chunk_words)
            
            # T·∫°o metadata chi ti·∫øt cho chunk
            chunk_metadata = {
                **metadata,
                'chunk_id': f"{doc_hash}_{chunk_num}",
                'chunk_index': chunk_num,
                'chunk_start_word': start_idx,
                'chunk_end_word': end_idx - 1,
                'chunk_size_words': len(chunk_words),
                'chunk_size_chars': len(chunk_text),
                'chunk_hash': hashlib.md5(chunk_text.encode()).hexdigest()[:8],
                'total_chunks': -1,  # S·∫Ω c·∫≠p nh·∫≠t sau
                'document_hash': doc_hash,
                'processed_at': datetime.now().isoformat()
            }
            
            chunks.append({
                'id': chunk_metadata['chunk_id'],
                'text': chunk_text,
                'metadata': chunk_metadata,
                'embedding_ready': True
            })
            
            # Di chuy·ªÉn start index cho chunk ti·∫øp theo
            start_idx = max(start_idx + 1, end_idx - self.chunk_overlap)
            chunk_num += 1
        
        # C·∫≠p nh·∫≠t total_chunks
        total_chunks = len(chunks)
        for chunk in chunks:
            chunk['metadata']['total_chunks'] = total_chunks
        
        logger.debug(f"Split into {total_chunks} chunks (chunk_size={self.chunk_size}, overlap={self.chunk_overlap})")
        return chunks
    
    def clean_and_chunk(self, raw_documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Pipeline ch√≠nh: L√†m s·∫°ch v√† chunk t·∫•t c·∫£ documents
        - Clean t·ª´ng document
        - Chunk document ƒë√£ clean
        - Thu th·∫≠p metadata

        Args:
            raw_documents: List dict, m·ªói dict c√≥ 'content' v√† optional 'metadata'

        Returns:
            List c√°c chunk ƒë√£ ƒë∆∞·ª£c clean v√† chunked
        """
        all_chunks = []
        total_docs = len(raw_documents)
        
        if total_docs == 0:
            logger.warning("No documents to process")
            return []
        
        logger.info(f"Starting clean_and_chunk for {total_docs} documents")
        
        for idx, raw_doc in enumerate(raw_documents):
            content = raw_doc.get('content', '')
            doc_metadata = raw_doc.get('metadata', {})
            
            if not content:
                logger.warning(f"Document {idx} has no content, skipping")
                continue
            
            # 1. B·ªï sung th√¥ng tin c∆° b·∫£n v√†o metadata
            enriched_metadata = self._enrich_metadata(raw_doc, doc_metadata, idx)
            
            # 2. Clean document
            try:
                cleaned_content = self.clean_text(content)
            except Exception as e:
                logger.error(f"Error cleaning document {idx}: {e}")
                cleaned_content = content  # Fallback to original
            
            # 3. Chunk document ƒë√£ clean
            chunks = self.split_into_chunks(cleaned_content, enriched_metadata)
            all_chunks.extend(chunks)
            
            # 4. Log ti·∫øn tr√¨nh
            if (idx + 1) % max(1, total_docs // 10) == 0 or (idx + 1) == total_docs:
                logger.info(f"Processed {idx + 1}/{total_docs} documents, "
                           f"created {len(all_chunks)} chunks so far")
        
        logger.info(f"‚úÖ Completed: {len(all_chunks)} chunks from {total_docs} documents")
        
        # Log th·ªëng k√™ chi ti·∫øt
        if all_chunks:
            self._log_statistics(all_chunks)
        
        return all_chunks
    
    def _enrich_metadata(self, raw_doc: Dict[str, Any], metadata: Dict[str, Any], idx: int) -> Dict[str, Any]:
        """B·ªï sung th√¥ng tin metadata t·ª´ raw_doc"""
        enriched = metadata.copy()
        
        # Source information
        if 'source' not in enriched:
            enriched['source'] = raw_doc.get('source', 'unknown')
        
        # URL information
        if 'url' not in enriched:
            url = raw_doc.get('url', '')
            if url:
                enriched['url'] = url
        
        # Title information
        if 'title' not in enriched:
            title = raw_doc.get('title') or raw_doc.get('file_name') or f'Document_{idx}'
            enriched['title'] = title
        
        # File information
        if 'file_name' not in enriched:
            file_name = raw_doc.get('file_name', '')
            if file_name:
                enriched['file_name'] = file_name
        
        # Original document index
        enriched['document_index'] = idx
        
        return enriched
    
    def _log_statistics(self, chunks: List[Dict[str, Any]]):
        """Log th·ªëng k√™ chi ti·∫øt v·ªÅ chunks"""
        if not chunks:
            return
        
        total_chunks = len(chunks)
        total_words = sum(len(chunk['text'].split()) for chunk in chunks)
        total_chars = sum(len(chunk['text']) for chunk in chunks)
        avg_words = total_words / total_chunks
        avg_chars = total_chars / total_chunks
        
        # Find min/max chunk sizes
        word_counts = [len(chunk['text'].split()) for chunk in chunks]
        char_counts = [len(chunk['text']) for chunk in chunks]
        
        logger.info("üìä Chunk Statistics:")
        logger.info(f"   Total chunks: {total_chunks:,}")
        logger.info(f"   Total words: {total_words:,}")
        logger.info(f"   Total characters: {total_chars:,}")
        logger.info(f"   Average words per chunk: {avg_words:.1f}")
        logger.info(f"   Average chars per chunk: {avg_chars:.1f}")
        logger.info(f"   Min/Max words: {min(word_counts)} / {max(word_counts)}")
        logger.info(f"   Min/Max chars: {min(char_counts)} / {max(char_counts)}")
        logger.info(f"   Config: chunk_size={self.chunk_size}, overlap={self.chunk_overlap}")
    
    def preprocess_documents(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Alias ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code c≈©
        
        Args:
            documents: List c√°c document raw
            
        Returns:
            List c√°c chunk ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        return self.clean_and_chunk(documents)


# Example usage
if __name__ == "__main__":
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Create preprocessor
    preprocessor = TextPreprocessor(
        chunk_size=800,
        chunk_overlap=150
    )
    
    # Example documents
    sample_documents = [
        {
            'content': """
            <html>
            <head><title>Company Contact</title></head>
            <body>
            <h1>ABC Corporation</h1>
            <p>For sales inquiries, please contact our Sales Department at sales@abccorp.com</p>
            <p>Phone: +1-800-123-4567</p>
            <p>Support email: support@abccorp.com</p>
            <p>Our offices are in USA, Germany, and Vietnam.</p>
            <script>console.log('test');</script>
            </body>
            </html>
            """,
            'metadata': {
                'source': 'website',
                'url': 'https://abccorp.com/contact'
            }
        },
        {
            'content': """
            Product Specifications Document
            Technical support is available 24/7.
            Contact Dr. John Smith for engineering questions.
            Regional offices in Southeast Asia and Europe.
            Email: info@company.com
            Phone: +84 28 3823 4567
            """,
            'file_name': 'product_specs.txt'
        }
    ]
    
    # Process documents
    chunks = preprocessor.clean_and_chunk(sample_documents)
    
    # Display results
    print(f"\nGenerated {len(chunks)} chunks:")
    for i, chunk in enumerate(chunks[:3]):  # Show first 3 chunks
        print(f"\n--- Chunk {i+1} ---")
        print(f"ID: {chunk['id']}")
        print(f"Text: {chunk['text'][:200]}...")
        print(f"Metadata keys: {list(chunk['metadata'].keys())}")