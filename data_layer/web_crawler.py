import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urldefrag
import logging
from typing import List, Dict, Set, Tuple
import time
import re
import json
import xml.etree.ElementTree as ET
from collections import deque
import hashlib
from datetime import datetime
from urllib.robotparser import RobotFileParser
import random
from .data_manager import DataManager

logger = logging.getLogger(__name__)

class EnhancedWebCrawler:
    """
    Crawler n√¢ng cao d√πng cho website Arbin
    --------------------------------------------------
    - H·ªó tr·ª£ crawl to√†n site (Arbin.com)
    - T·ª± ƒë·ªông ph√°t hi·ªán sitemap, l·ªçc URL theo robots.txt
    - C√≥ h·ªá th·ªëng ch·∫•m ƒëi·ªÉm ƒë·ªô quan tr·ªçng (importance score)
    - T·ª± ƒë·ªông nh·∫≠n d·∫°ng email, s·ªë ƒëi·ªán tho·∫°i
    - H·ªó tr·ª£ recrawl th√¥ng minh theo th·ªùi gian
    - T√≠ch h·ª£p l∆∞u tr·ªØ v·ªõi DataManager
    """

    def __init__(self, base_url: str = "https://www.arbin.com/"):
        self.base_url = base_url
        self.base_domain = urlparse(base_url).netloc
        self.visited_urls: Set[str] = set()
        self.urls_to_visit = deque()
        self.data_manager = DataManager()
        self.data_key = "enhanced_web_crawler"
        # T·∫≠p h·ª£p email v√† hotline ph√°t hi·ªán ƒë∆∞·ª£c
        self.emails: Set[str] = set()
        self.hotlines: Set[str] = set()
        self.crawl_stats = {
            'total': 0,
            'successful': 0,
            'failed': 0,
            'failed_urls': []  # L∆∞u c√°c URL th·∫•t b·∫°i
        }

        # B·ªô nh·ªõ theo d√µi URL ƒë√£ x·ª≠ l√Ω (hash n·ªôi dung + timestamp)
        self.processed_urls_info: Dict[str, Dict] = {}
        self.load_previous_crawls()

        # Thi·∫øt l·∫≠p session HTTP chung ƒë·ªÉ t√°i s·ª≠ d·ª•ng k·∫øt n·ªëi
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })

        # ƒê·ªçc v√† ki·ªÉm tra robots.txt
        self.robot_parser = RobotFileParser()
        self.robot_parser.set_url(urljoin(self.base_url, "robots.txt"))
        try:
            self.robot_parser.read()
        except:
            logger.warning("Could not read robots.txt")

        # Gi·ªõi h·∫°n v√† c·∫•u h√¨nh crawl
        self.max_pages = 500      # Gi·ªõi h·∫°n s·ªë trang t·ªëi ƒëa
        self.max_depth = 5        # M·ª©c s√¢u t·ªëi ƒëa khi ƒë·ªá quy theo link
        self.delay = 0.5          # Th·ªùi gian ngh·ªâ gi·ªØa c√°c request
        self.timeout = 15         # Timeout m·ªói request

        # C·∫•u h√¨nh s·ªë ng√†y recrawl cho t·ª´ng lo·∫°i trang
        self.recrawl_intervals = {
            'homepage': 1,
            'product_pages': 7,
            'support_pages': 30,
            'documentation': 90,
            'default': 14
        }

        # C√°c pattern ∆∞u ti√™n crawl (li√™n quan t·ªõi s·∫£n ph·∫©m / k·ªπ thu·∫≠t)
        self.priority_patterns = [
            r'/products/', r'/software/', r'/support/', r'/resources/', r'/applications/',
            r'/technical[-_]?support/', r'/downloads?/', r'/documentation/', r'/manuals?/',
            r'/specifications?/', r'/datasheets?/', r'/brochures?/', r'/news/', r'/blog/',
            r'/articles?/', r'/tutorials?/', r'/guides?/', r'/faqs?/', r'/help/', r'/knowledge[-_]?base/'
        ]

        # C√°c pattern c·∫ßn lo·∫°i tr·ª´ khi crawl
        self.exclude_patterns = [
            r'\.(css|js)$', r'\/cart\/', r'\/checkout\/', r'\/account\/', r'\/login\/',
            r'\/register\/', r'\/wp-admin\/', r'\/wp-content\/', r'\/wp-json\/',
            r'\/wp-includes\/', r'\/cgi-bin\/', r'\?', r'\.php$', r'\.asp$', r'\.aspx$',
            r'#', r'javascript:', r'mailto:', r'tel:'
        ]

        # T·∫≠p h·ª£p t·ª´ kh√≥a k·ªπ thu·∫≠t quan tr·ªçng ƒë·ªÉ t√≠nh ƒëi·ªÉm n·ªôi dung
        self.important_keywords = [
            'arbin', 'battery', 'test', 'testing', 'system', 'bt-', 'lbt', 'mbt', 'mits',
            'software', 'pro', 'hardware', 'specification', 'technical', 'data', 'measurement',
            'voltage', 'current', 'capacity', 'cycler', 'tester', 'ev', 'electric', 'vehicle',
            'r&d', 'research', 'development', 'manufacturing', 'quality', 'control',
            'laboratory', 'lab', 'cell', 'lithium', 'ion', 'battery', 'analysis', 'daq',
            'acquisition', 'calibration', 'accuracy', 'resolution', 'channel', 'module', 'modular',
            'configuration'
        ]

    # -----------------------------------
    # Load d·ªØ li·ªáu crawl tr∆∞·ªõc ƒë√≥ ƒë·ªÉ tr√°nh tr√πng l·∫∑p
    # -----------------------------------
    def load_previous_crawls(self):
        try:
            previous_data = self.data_manager.load_raw_data(self.data_key)
            if previous_data:
                for doc in previous_data:
                    url = doc.get('url')
                    if url:
                        self.processed_urls_info[url] = {
                            'content_hash': self.get_content_hash(doc.get('content', '')),
                            'crawled_at': doc.get('crawled_at', ''),
                            'title': doc.get('title', ''),
                            'content_length': len(doc.get('content', ''))
                        }
                logger.info(f"Loaded {len(self.processed_urls_info)} previously crawled URLs")
        except Exception as e:
            logger.warning(f"Could not load previous crawls: {e}")

    # -----------------------------------
    # Ti·ªán √≠ch x·ª≠ l√Ω URL & n·ªôi dung
    # -----------------------------------
    def get_content_hash(self, content: str) -> str:
        """Sinh m√£ bƒÉm MD5 ƒë·ªÉ ki·ªÉm tra n·ªôi dung thay ƒë·ªïi"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()

    def normalize_url(self, url: str) -> str:
        """Chu·∫©n h√≥a URL: b·ªè fragment, c·∫Øt / d∆∞ th·ª´a"""
        url = urldefrag(url)[0]
        parsed = urlparse(url)
        path = parsed.path.rstrip('/')
        return parsed._replace(path=path).geturl()

    def check_robots_permission(self, url: str) -> bool:
        """Ki·ªÉm tra quy·ªÅn crawl theo robots.txt"""
        return self.robot_parser.can_fetch("*", url)

    def should_crawl_url(self, url: str, parent_url: str = None) -> bool:
        """Quy·∫øt ƒë·ªãnh c√≥ n√™n crawl URL hay kh√¥ng (l·ªçc domain, lo·∫°i tr·ª´ pattern x·∫•u)"""
        url = self.normalize_url(url)
        parsed = urlparse(url)
        if parsed.netloc and parsed.netloc != self.base_domain:
            return False
        if not self.check_robots_permission(url):
            return False
        url_lower = url.lower()
        for pattern in self.exclude_patterns:
            if re.search(pattern, url_lower, re.IGNORECASE):
                return False
        return True

    # -----------------------------------
    # Logic x√°c ƒë·ªãnh t√°i crawl (Recrawl)
    # -----------------------------------
    def get_recrawl_interval(self, url: str) -> int:
        """Tr·∫£ v·ªÅ s·ªë ng√†y gi·ªØa 2 l·∫ßn crawl cho t·ª´ng lo·∫°i trang"""
        url_lower = url.lower()
        if url == self.base_url:
            return self.recrawl_intervals['homepage']
        for pattern, days in [
            (r'/products/', self.recrawl_intervals['product_pages']),
            (r'/software/', self.recrawl_intervals['product_pages']),
            (r'/support/', self.recrawl_intervals['support_pages']),
            (r'/technical[-_]?support/', self.recrawl_intervals['support_pages']),
            (r'/documentation/', self.recrawl_intervals['documentation']),
            (r'/manuals?/', self.recrawl_intervals['documentation']),
        ]:
            if re.search(pattern, url_lower):
                return days
        return self.recrawl_intervals['default']

    def should_recrawl_url(self, url: str) -> Tuple[bool, str]:
        """
        X√°c ƒë·ªãnh c√≥ c·∫ßn t√°i crawl URL kh√¥ng
        - D·ª±a v√†o ng√†y crawl c≈© + kho·∫£ng th·ªùi gian ƒë·ªãnh nghƒ©a
        """
        url = self.normalize_url(url)
        if url not in self.processed_urls_info:
            return True, "New URL"

        info = self.processed_urls_info[url]
        crawled_at = info.get('crawled_at', '')
        if not crawled_at:
            return True, "No previous crawl timestamp"
        try:
            prev_time = datetime.strptime(crawled_at, "%Y-%m-%d %H:%M:%S")
            days_since = (datetime.now() - prev_time).days
            recrawl_days = self.get_recrawl_interval(url)
            if days_since >= recrawl_days:
                return True, f"Due for recrawl ({days_since}/{recrawl_days} days)"
            else:
                return False, f"Recently crawled ({days_since}/{recrawl_days} days ago)"
        except Exception as e:
            logger.warning(f"Error parsing crawl time for {url}: {e}")
            return True, "Invalid timestamp format"

    # -----------------------------------
    # Tr√≠ch xu·∫•t li√™n k·∫øt v√† n·ªôi dung
    # -----------------------------------
    def extract_links(self, soup: BeautifulSoup, current_url: str) -> List[Tuple[str, str]]:
        """L·∫•y t·∫•t c·∫£ c√°c li√™n k·∫øt h·ª£p l·ªá trong trang"""
        links = set()
        for a_tag in soup.find_all('a', href=True):
            href = urljoin(current_url, a_tag['href'])
            href = self.normalize_url(href)
            if self.should_crawl_url(href, current_url):
                links.add((href, a_tag.get_text(strip=True)))
        return list(links)

    def extract_priority_content(self, soup: BeautifulSoup, url: str) -> Dict[str, any]:
        """
        L·∫•y ph·∫ßn n·ªôi dung ch√≠nh (main content) t·ª´ c√°c selector ph·ªï bi·∫øn
        ∆Øu ti√™n div/article ch√≠nh, fallback l√† to√†n b·ªô text
        """
        content = {}
        content_selectors = [
            ('main', 'main'), ('article', 'article'), ('div[class*="content"]', 'content_div'),
            ('div[class*="main"]', 'main_div'), ('div[class*="post"]', 'post'),
            ('div[class*="entry"]', 'entry'), ('section', 'section'),
            ('div[class*="body"]', 'body'), ('div[class*="text"]', 'text'),
            ('div[class*="description"]', 'description'), ('div[class*="wp-block-"]', 'wp_block'),
            ('div[class*="et_pb_text"]', 'et_text'), ('div[class*="et_pb_module"]', 'et_module')
        ]
        for selector, name in content_selectors:
            elements = soup.select(selector)
            for i, elem in enumerate(elements[:3]):
                text = elem.get_text(separator=' ', strip=True)
                if text and len(text) > 100:
                    content[f"{name}_{i}"] = {
                        'text': text,
                        'selector': selector,
                        'length': len(text)
                    }
        if not content:
            all_text = soup.get_text(separator=' ', strip=True)
            content['full_page'] = {'text': all_text, 'selector': 'full_page', 'length': len(all_text)}
        return content

    def extract_metadata(self, soup: BeautifulSoup, url: str) -> Dict[str, any]:
        """Tr√≠ch xu·∫•t metadata nh∆∞ title, description, headers, link/image count"""
        metadata = {'title': '', 'description': '', 'keywords': [], 'headers': {}, 'links_count': 0, 'images_count': 0}
        if soup.title and soup.title.string:
            metadata['title'] = soup.title.string.strip()
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            metadata['description'] = meta_desc['content'].strip()
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords and meta_keywords.get('content'):
            metadata['keywords'] = [k.strip() for k in meta_keywords['content'].split(',')]
        for i in range(1, 7):
            headers = soup.find_all(f'h{i}')
            if headers:
                metadata['headers'][f'h{i}'] = [h.get_text(strip=True) for h in headers[:5]]
        metadata['links_count'] = len(soup.find_all('a', href=True))
        metadata['images_count'] = len(soup.find_all('img'))
        return metadata

    def analyze_content_importance(self, text: str, url: str) -> float:
        """
        T√≠nh ƒëi·ªÉm ƒë·ªô quan tr·ªçng c·ªßa n·ªôi dung
        - D·ª±a tr√™n s·ªë t·ª´ kh√≥a k·ªπ thu·∫≠t v√† pattern ∆∞u ti√™n
        - C·ªông ƒëi·ªÉm cho n·ªôi dung d√†i
        """
        importance_score = 0
        text_lower = text.lower()
        for keyword in self.important_keywords:
            if keyword.lower() in text_lower:
                importance_score += 1
        url_lower = url.lower()
        for pattern in self.priority_patterns:
            if re.search(pattern, url_lower, re.IGNORECASE):
                importance_score += 3
        if len(text) > 1000:
            importance_score += 2
        elif len(text) > 500:
            importance_score += 1
        return importance_score

    # ------------------------------
    # H√†m crawl t·ª´ng trang
    # ------------------------------
    def crawl_page(self, url: str, depth: int = 0, force_recrawl: bool = False, parent_url: str = None, link_text: str = "") -> Dict[str, any]:
        """
        Crawl 1 trang ƒë∆°n l·∫ª:
        - G·ªçi HTTP GET
        - L√†m s·∫°ch n·ªôi dung
        - T√≠nh hash & ƒëi·ªÉm quan tr·ªçng
        - Tr√≠ch xu·∫•t email, phone
        - Th√™m c√°c link m·ªõi v√†o h√†ng ƒë·ª£i
        """
        self.crawl_stats['total'] += 1
        url = self.normalize_url(url)
        if not force_recrawl and url in self.processed_urls_info:
            should_recrawl, reason = self.should_recrawl_url(url)
            if not should_recrawl:
                # N·∫øu URL g·∫ßn ƒë√¢y ƒë√£ crawl ‚Üí b·ªè qua (tr·∫£ cached)
                info = self.processed_urls_info[url]
                return {
                    'url': url, 'title': info.get('title', ''), 'content': '', 'source': 'web',
                    'source_type': 'web', 'depth': depth, 'crawled_at': info['crawled_at'],
                    'content_length': info.get('content_length', 0), 'importance_score': 0,
                    'status_code': 304, 'cached': True, 'previous_hash': info.get('content_hash', ''),
                    'recrawl_reason': reason
                }

        if url in self.visited_urls:
            return {}

        # N·∫øu l√† file PDF ‚Üí ch·ªâ l∆∞u metadata, kh√¥ng crawl n·ªôi dung
        if url.lower().endswith('.pdf'):
            self.data_manager.save_file_metadata({'url': url, 'type': 'pdf', 'source_page': parent_url})
            return {}

        logger.info(f"üîç Crawling: {url} (depth: {depth})")
        self.visited_urls.add(url)

        # Delay ng·∫´u nhi√™n nh·∫π ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
        time.sleep(self.delay + random.uniform(0.1, 0.3))

        # C∆° ch·∫ø retry 3 l·∫ßn n·∫øu request th·∫•t b·∫°i
        for attempt in range(3):
            try:
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                self.crawl_stats['successful'] += 1
                break
            except requests.RequestException as e:
                logger.warning(f"Attempt {attempt+1} failed for {url}: {e}")
                if attempt == 2:  # L·∫ßn th·ª≠ cu·ªëi c√πng th·∫•t b·∫°i
                    self.crawl_stats['failed'] += 1
                    self.crawl_stats['failed_urls'].append({
                        'url': url,
                        'error': str(e),
                        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        'depth': depth,
                        'parent_url': parent_url
                    })
                time.sleep(2 ** attempt)
        else:
            logger.error(f"Failed all retries: {url}")
            return {}

        # B·ªè qua n·∫øu kh√¥ng ph·∫£i HTML
        content_type = response.headers.get('Content-Type', '').lower()
        if 'text/html' not in content_type:
            return {}

        # D√≤ encoding v√† parse HTML
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, 'html.parser')

        # Lo·∫°i b·ªè c√°c ph·∫ßn kh√¥ng c·∫ßn thi·∫øt
        for selector in ['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'iframe', 'noscript', 'svg']:
            for element in soup.select(selector):
                element.decompose()

        # Tr√≠ch xu·∫•t n·ªôi dung v√† metadata
        content_data = self.extract_priority_content(soup, url)
        metadata = self.extract_metadata(soup, url)

        # G·ªôp to√†n b·ªô text l·∫°i
        combined_text = ' '.join([d['text'] for d in content_data.values()])
        cleaned_text = ' '.join(chunk.strip() for line in combined_text.splitlines() for chunk in line.split("  ") if chunk.strip())

        importance_score = self.analyze_content_importance(cleaned_text, url)
        content_hash = self.get_content_hash(cleaned_text)
        previous_hash = self.processed_urls_info.get(url, {}).get('content_hash')
        is_updated = previous_hash != content_hash if previous_hash else False

        # Tr√≠ch xu·∫•t email v√† s·ªë ƒëi·ªán tho·∫°i
        emails = list(set(re.findall(r"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+", cleaned_text)))
        phones = list(set(re.findall(r"(\+?\d{1,3})?[\s.-]?\(?\d{2,4}\)?[\s.-]?\d{3,4}[\s.-]?\d{3,4}", cleaned_text)))

        # K·∫øt qu·∫£ crawl 1 trang
        result = {
            'url': url,
            'title': metadata['title'],
            'content': cleaned_text,
            'source': 'web',
            'source_type': 'web',
            'depth': depth,
            'crawled_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'content_length': len(cleaned_text),
            'importance_score': importance_score,
            'status_code': response.status_code,
            'metadata': metadata,
            'content_sections': content_data,
            'content_hash': content_hash,
            'is_updated': is_updated,
            'previous_hash': previous_hash,
            'parent_url': parent_url,
            'link_text': link_text,
            'emails': emails,
            'phones': phones
        }

        # C·∫≠p nh·∫≠t th√¥ng tin URL ƒë√£ x·ª≠ l√Ω
        self.processed_urls_info[url] = {
            'content_hash': content_hash,
            'crawled_at': result['crawled_at'],
            'title': result['title'],
            'content_length': result['content_length']
        }

        # N·∫øu ch∆∞a ƒë·∫°t max_depth ‚Üí th√™m link m·ªõi v√†o queue
        if depth < self.max_depth:
            links = self.extract_links(soup, url)
            for link, text in links:
                if link not in self.visited_urls and link not in [item[0] for item in self.urls_to_visit]:
                    self.urls_to_visit.append((link, depth + 1, False, url, text))

        logger.info(f"Crawled: {url} - {len(cleaned_text)} chars, score: {importance_score}, updated: {is_updated}, emails: {len(emails)}, phones: {len(phones)}")
        return result

    # ------------------------------
    # X·ª≠ l√Ω Sitemap
    # c√≥ nhi·ªám v·ª• t·ª± ƒë·ªông ph√°t hi·ªán v√† ƒë·ªçc danh s√°ch URL c√≥ s·∫µn t·ª´ sitemap c·ªßa website, 
    # thay v√¨ ph·∫£i t√¨m link th·ªß c√¥ng b·∫±ng c√°ch crawl t·ª´ng trang m·ªôt.
    # ------------------------------
    def discover_sitemaps(self) -> List[str]:
        """T√¨m t·∫•t c·∫£ sitemap c√≥ th·ªÉ t·ª´ robots.txt v√† c√°c t√™n ph·ªï bi·∫øn"""
        sitemap_urls = []
        common_paths = [
            'sitemap.xml', 'sitemap_index.xml', 'sitemap1.xml',
            'sitemap-news.xml', 'sitemap-products.xml', 'sitemap-articles.xml', 'robots.txt'
        ]
        for path in common_paths:
            sitemap_url = urljoin(self.base_url, path)
            try:
                response = self.session.head(sitemap_url, timeout=5)
                if response.status_code == 200:
                    sitemap_urls.append(sitemap_url)
            except:
                continue

        # Ki·ªÉm tra robots.txt ƒë·ªÉ t√¨m d√≤ng "Sitemap:"
        robots_url = urljoin(self.base_url, 'robots.txt')
        try:
            response = self.session.get(robots_url, timeout=5)
            if response.status_code == 200:
                for line in response.text.splitlines():
                    if line.lower().startswith('sitemap:'):
                        sitemap_url = line.split(':', 1)[1].strip()
                        sitemap_urls.append(urljoin(self.base_url, sitemap_url))
        except:
            pass
        return list(set(sitemap_urls))

    def parse_sitemap(self, sitemap_url: str) -> List[str]:
        """Ph√¢n t√≠ch sitemap XML v√† tr√≠ch xu·∫•t danh s√°ch URL"""
        urls = []
        try:
            response = self.session.get(sitemap_url, timeout=10)
            if 'xml' in response.headers.get('Content-Type', ''):
                try:
                    root = ET.fromstring(response.content)
                    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
                    url_tags = root.findall('.//ns:url/ns:loc', namespace) or root.findall('.//loc')
                    for url_tag in url_tags:
                        url = url_tag.text.strip()
                        if url and self.should_crawl_url(url):
                            urls.append(url)
                except ET.ParseError:
                    # N·∫øu XML l·ªói ‚Üí fallback sang BeautifulSoup
                    soup = BeautifulSoup(response.content, 'xml')
                    url_tags = soup.find_all('loc')
                    for url_tag in url_tags:
                        url = url_tag.text.strip()
                        if url and self.should_crawl_url(url):
                            urls.append(url)

            logger.info(f"Parsed {len(urls)} URLs from sitemap: {sitemap_url}")
        except Exception as e:
            logger.error(f"Error parsing sitemap {sitemap_url}: {e}")
        return urls
    
    def print_simple_statistics(self):
        """Hi·ªÉn th·ªã th·ªëng k√™ ƒë∆°n gi·∫£n v·ªÅ k·∫øt qu·∫£ crawl"""
        print("\n" + "="*60)
        print("üìä TH·ªêNG K√ä CRAWL")
        print("="*60)
        
        total = self.crawl_stats['total']
        if total == 0:
            print("‚ùå Ch∆∞a th·ª±c hi·ªán crawl n√†o")
            return
            
        successful = self.crawl_stats['successful']
        failed = self.crawl_stats['failed']
        cached = total - successful - failed  # URL ƒë∆∞·ª£c cache
        
        success_rate = (successful / total * 100) if total > 0 else 0
        failure_rate = (failed / total * 100) if total > 0 else 0
        cache_rate = (cached / total * 100) if total > 0 else 0
        
        print(f"üìà T·ªîNG S·ªê URL X·ª¨ L√ù: {total}")
        print(f"‚úÖ TH√ÄNH C√îNG: {successful} ({success_rate:.1f}%)")
        print(f"‚ùå TH·∫§T B·∫†I: {failed} ({failure_rate:.1f}%)")
        print(f"üíæ ƒê√É CACHE (kh√¥ng thay ƒë·ªïi): {cached} ({cache_rate:.1f}%)")
        print()
        
        # Hi·ªÉn th·ªã c√°c URL th·∫•t b·∫°i n·∫øu c√≥
        if self.crawl_stats['failed_urls']:
            print("üî¥ C√ÅC URL TH·∫§T B·∫†I:")
            for i, failed_url in enumerate(self.crawl_stats['failed_urls'][:10], 1):
                print(f"  {i}. {failed_url['url']}")
                print(f"     L·ªói: {failed_url['error'][:100]}...")
                print(f"     ƒê·ªô s√¢u: {failed_url['depth']}")
                if failed_url['parent_url']:
                    print(f"     T·ª´ trang: {failed_url['parent_url']}")
                print()
            
            if len(self.crawl_stats['failed_urls']) > 10:
                print(f"  ... v√† {len(self.crawl_stats['failed_urls']) - 10} URL th·∫•t b·∫°i kh√°c")
        
        print("="*60)

    def get_statistics_summary(self) -> Dict[str, any]:
        """Tr·∫£ v·ªÅ t√≥m t·∫Øt th·ªëng k√™ d·∫°ng dictionary"""
        total = self.crawl_stats['total']
        successful = self.crawl_stats['successful']
        failed = self.crawl_stats['failed']
        cached = total - successful - failed
        
        return {
            'total_urls': total,
            'successful': successful,
            'failed': failed,
            'cached': cached,
            'success_rate': (successful / total * 100) if total > 0 else 0,
            'failure_rate': (failed / total * 100) if total > 0 else 0,
            'cache_rate': (cached / total * 100) if total > 0 else 0,
            'failed_urls': self.crawl_stats['failed_urls'],
            'failed_count': len(self.crawl_stats['failed_urls'])
        }

    # -----------------------------------
    # Crawl to√†n site
    # -----------------------------------
    def get_initial_urls(self, force_recrawl: bool = False) -> List[str]:
        """L·∫•y danh s√°ch URL kh·ªüi t·∫°o (∆∞u ti√™n sitemap, sau ƒë√≥ l√† c√°c section ch√≠nh)"""
        initial_urls = []
        sitemaps = self.discover_sitemaps()
        for sitemap in sitemaps:
            urls = self.parse_sitemap(sitemap)
            if not force_recrawl:
                filtered = []
                for url in urls[:100]:
                    should_recrawl, _ = self.should_recrawl_url(url)
                    if should_recrawl:
                        filtered.append(url)
                initial_urls.extend(filtered)
            else:
                initial_urls.extend(urls[:100])

        # N·∫øu kh√¥ng t√¨m th·∫•y sitemap ‚Üí fallback v√†o homepage v√† c√°c section ch√≠nh
        if not initial_urls:
            should_recrawl, _ = self.should_recrawl_url(self.base_url)
            if force_recrawl or should_recrawl:
                initial_urls.append(self.base_url)
            important_sections = ['/products/', '/software/', '/support/', '/resources/', '/applications/', '/about/', '/news/', '/blog/']
            for section in important_sections:
                url = urljoin(self.base_url, section)
                should_recrawl, _ = self.should_recrawl_url(url)
                if force_recrawl or should_recrawl:
                    initial_urls.append(url)

        # Ch·∫•m ƒëi·ªÉm ∆∞u ti√™n theo pattern
        prioritized = []
        for url in initial_urls:
            priority = sum(2 for pattern in self.priority_patterns if re.search(pattern, url, re.IGNORECASE))
            if url in self.processed_urls_info:
                priority -= 1
            prioritized.append((url, priority))
        prioritized.sort(key=lambda x: x[1], reverse=True)
        return [u for u, _ in prioritized[:50]]

    def crawl_site(self, force_recrawl: bool = False) -> List[Dict[str, any]]:
        """
        H√†m ƒëi·ªÅu ph·ªëi crawl to√†n b·ªô website
        - Qu·∫£n l√Ω queue
        - Ghi log ti·∫øn tr√¨nh
        - L∆∞u k·∫øt qu·∫£ ƒë·ªãnh k·ª≥
        """
        self.crawl_stats = {
            'total': 0,
            'successful': 0,
            'failed': 0,
            'failed_urls': []
        }
        logger.info(f"Starting {'forced ' if force_recrawl else 'incremental '}crawl of {self.base_url}")
        self.visited_urls.clear()
        self.urls_to_visit.clear()
        documents = []
        new_count = updated_count = cached_count = error_count = 0

        # L·∫•y danh s√°ch URL b·∫Øt ƒë·∫ßu
        initial_urls = self.get_initial_urls(force_recrawl)
        for url in initial_urls:
            self.urls_to_visit.append((url, 0, force_recrawl, None, ""))

        while self.urls_to_visit and len(documents) < self.max_pages:
            url, depth, force, parent, link_text = self.urls_to_visit.popleft()
            if url in self.visited_urls:
                continue
            try:
                doc = self.crawl_page(url, depth, force, parent, link_text)
                if not doc:
                    continue

                # Ph√¢n lo·∫°i k·∫øt qu·∫£
                if doc.get('status_code') == 304:
                    cached_count += 1
                elif doc.get('content'):
                    documents.append(doc)
                    if doc.get('is_updated', False):
                        updated_count += 1
                    else:
                        new_count += 1
                else:
                    error_count += 1

                # Log ti·∫øn tr√¨nh v·ªõi th·ªëng k√™ c∆° b·∫£n
                if len(documents) % 10 == 0 or len(documents) == 1:
                    total_processed = self.crawl_stats['total']
                    successful = self.crawl_stats['successful']
                    failed = self.crawl_stats['failed']
                    success_rate = (successful / total_processed * 100) if total_processed > 0 else 0
                    
                    logger.info(
                        f"üìä Progress: {len(documents)}/{self.max_pages} pages | "
                        f"Success rate: {success_rate:.1f}% | "
                        f"New: {new_count} | Updated: {updated_count} | "
                        f"Cached: {cached_count} | Errors: {error_count}"
                    )

                # Backup queue m·ªói 50 trang
                if len(documents) % 50 == 0:
                    self.data_manager.save_raw_data(list(self.urls_to_visit), "crawl_queue_backup")

            except Exception as e:
                logger.error(f"Error crawling {url}: {e}")
                error_count += 1
                self.crawl_stats['failed'] += 1
                self.crawl_stats['failed_urls'].append({
                    'url': url,
                    'error': str(e),
                    'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'depth': depth,
                    'parent_url': parent
                })

        # Sau khi crawl xong ‚Üí l∆∞u k·∫øt qu·∫£
        if documents:
            self.save_crawl_results(documents)

        # Hi·ªÉn th·ªã th·ªëng k√™ cu·ªëi c√πng
        self.print_simple_statistics()
        
        # L∆∞u th·ªëng k√™ v√†o file
        self.save_statistics_to_file()

        logger.info(f"‚úÖ Crawling completed. Total: {len(documents)} pages, New: {new_count}, Updated: {updated_count}, Cached: {cached_count}, Errors: {error_count}")

        
        return documents
    def save_statistics_to_file(self):
        """L∆∞u th·ªëng k√™ v√†o file JSON"""
        stats = self.get_statistics_summary()
        stats['base_url'] = self.base_url
        stats['crawl_completed_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        filename = f"crawl_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = f"./data/inspection/{filename}"
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            logger.info(f"Statistics saved to {filepath}")
        except Exception as e:
            logger.error(f"Could not save statistics: {e}")

    # -----------------------------------
    # L∆∞u k·∫øt qu·∫£ crawl
    # -----------------------------------
    def save_crawl_results(self, documents: List[Dict[str, any]]):
        """
        L∆∞u to√†n b·ªô k·∫øt qu·∫£ crawl + xu·∫•t th·ªëng k√™
        Bao g·ªìm:
        - S·ªë l∆∞·ª£ng trang m·ªõi, c·∫≠p nh·∫≠t, cache
        - Ph√¢n ph·ªëi ƒëi·ªÉm quan tr·ªçng
        - 3 m·∫´u n·ªôi dung preview
        """
        try:
            self.data_manager.save_raw_data(documents, self.data_key)

            new_docs = [d for d in documents if not d.get('cached') and not d.get('is_updated')]
            updated_docs = [d for d in documents if d.get('is_updated')]
            cached_docs = [d for d in documents if d.get('cached')]

            stats = {
                'total_pages_crawled': len(documents),
                'new_documents': len(new_docs),
                'updated_documents': len(updated_docs),
                'cached_documents': len(cached_docs),
                'total_content_chars': sum(len(d.get('content', '')) for d in documents),
                'avg_content_length': (
                    sum(len(d.get('content', '')) for d in documents) / len(documents) if documents else 0
                ),
                'max_depth': max(d.get('depth', 0) for d in documents),
                'processed_urls_total': len(self.processed_urls_info),
                'importance_score_distribution': {
                    'high': len([d for d in documents if d.get('importance_score', 0) > 5]),
                    'medium': len([d for d in documents if 2 < d.get('importance_score', 0) <= 5]),
                    'low': len([d for d in documents if d.get('importance_score', 0) <= 2])
                },
                'top_new_urls': [d['url'] for d in new_docs[:5]],
                'top_updated_urls': [d['url'] for d in updated_docs[:5]]
            }

            # M·∫´u preview n·ªôi dung
            sample_content = []
            for doc in documents[:3]:
                sample_content.append({
                    'url': doc['url'],
                    'title': doc.get('title', ''),
                    'status': 'cached' if doc.get('cached') else 'updated' if doc.get('is_updated') else 'new',
                    'content_preview': (doc.get('content', '')[:300] + '...')
                    if len(doc.get('content', '')) > 300 else doc.get('content', ''),
                    'importance_score': doc.get('importance_score', 0),
                    'content_length': len(doc.get('content', ''))
                })
            stats['sample_content'] = sample_content

            stats['crawl_success_rate'] = self.get_statistics_summary()
            # Xu·∫•t th·ªëng k√™ ra file JSON inspection
            self.data_manager.export_for_inspection(stats, "enhanced_crawling_stats", "json")

            logger.info(f"Crawling Summary:")
            logger.info(f"  - Total pages: {stats['total_pages_crawled']}")
            logger.info(f"  - New documents: {stats['new_documents']}")
            logger.info(f"  - Updated documents: {stats['updated_documents']}")
            logger.info(f"  - Cached documents: {stats['cached_documents']}")
            logger.info(f"  - Total processed URLs: {stats['processed_urls_total']}")
        except Exception as e:
            logger.error(f"Error saving crawl results: {e}")

# Gi·ªØ alias c≈© ƒë·ªÉ t∆∞∆°ng th√≠ch ng∆∞·ª£c
WebCrawler = EnhancedWebCrawler
